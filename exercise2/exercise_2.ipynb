{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Exercise 2: Shape Classification and Segmentation\n",
    "\n",
    "**Submission Deadline**: 02.12.2025, 23:55\n",
    "\n",
    "In this exercise, we will dive into Machine Learning on 3D shapes by taking a look at shape classification and segmentation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0. Running this notebook\n",
    "We recommend running this notebook on a CUDA compatible local gpu. You can also run training on cpu, it will just take longer.\n",
    "\n",
    "You have two options for running this exercise on a GPU, choose one of them and start the exercise below in section \"Imports\":\n",
    "1. Locally on your own GPU/CPU\n",
    "2. On Google Colab\n",
    "\n",
    "We describe every option in more detail below:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### (a) Local Execution\n",
    "\n",
    "If you run this notebook locally, you have to first install the python dependiencies again. They are the same as for exercise 1 so you can re-use the environment you used last time. If you use [poetry](https://python-poetry.org), you can also simply re-install everything (`poetry install`) and then run this notebook via `poetry run jupyter notebook`.\n",
    "\n",
    "In case you are working with a RTX 3000-series GPU, you need to install a patched version of pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu113"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Google Colab\n",
    "\n",
    "If you don't have access to a GPU, you can also use Google Colab. However, we experienced the issue that inline visualization of shapes or inline images didn't work on colab, so just keep that in mind.\n",
    "What you can also do is only train networks on colab, download the checkpoint, and visualize inference locally.\n",
    "\n",
    "In case you're using Google Colab, you can upload the exercise folder (containing `exercise_2.ipynb`, directory `exercise_2` and the file `requirements.txt`) as `3d-machine-learning` to google drive (make sure you don't upload extracted datasets files).\n",
    "Additionally you'd need to open the notebook `exercise_2.ipynb` in Colab using `File > Open Notebook > Upload`.\n",
    "\n",
    "Next you'll need to run these two cells for setting up the environment. Before you do that make sure your instance has a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# # We assume you uploaded the exercise folder in root Google Drive folder\n",
    "\n",
    "# !cp -r /content/drive/MyDrive/3d-machine-learning 3d-machine-learning/\n",
    "# os.chdir('/content/3d-machine-learning/')\n",
    "# print('Installing requirements')\n",
    "# %pip install -r requirements.txt\n",
    "\n",
    "# # Make sure you restart runtime when directed by Colab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell after restarting your colab runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# import torch\n",
    "# os.chdir('/content/3d-machine-learning/')\n",
    "# sys.path.insert(1, \"/content/3d-machine-learning/\")\n",
    "# print('CUDA availability:', torch.cuda.is_available())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Imports\n",
    "\n",
    "The following imports should work regardless of whether you are using Colab or local execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import k3d\n",
    "import trimesh\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the next cell to test whether a GPU was detected by pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. ShapeNet Terms of Use"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide pre-processed shapes from the [ShapeNet](//shapenet.org) database for this exercise. ShapeNet is an ongoing effort to establish a richly-annotated, large-scale dataset of 3D shapes and is a collaborative effort between researchers at Princeton, Stanford and TTIC.\n",
    "\n",
    "<img src=\"exercise_2/images/shapenet.png\" alt=\"shapenet\" style=\"width: 512px;\"/>\n",
    "\n",
    "In order to be able to use the data, we ask you to read and agree to their Terms of Use as stated below (this is a requirement for passing this exercise):\n",
    "\n",
    "1. Researcher shall use the Database only for non-commercial research and educational purposes.\n",
    "2. Princeton University and Stanford University make no representations or warranties regarding the Database, including but not limited to warranties of non-infringement or fitness for a particular purpose.\n",
    "3. Researcher accepts full responsibility for his or her use of the Database and shall defend and indemnify Princeton University and Stanford University, including their employees, Trustees, officers and agents, against any and all claims arising from Researcher's use of the Database, including but not limited to Researcher's use of any copies of copyrighted 3D models that he or she may create from the Database.\n",
    "4. Researcher may provide research associates and colleagues with access to the Database provided that they first agree to be bound by these terms and conditions.\n",
    "5. Princeton University and Stanford University reserve the right to terminate Researcher's access to the Database at any time.\n",
    "6. If Researcher is employed by a for-profit, commercial entity, Researcher's employer shall also be bound by these terms and conditions, and Researcher hereby represents that he or she is fully authorized to enter into this agreement on behalf of such employer.\n",
    "7. The law of the State of New Jersey shall apply to all disputes under this agreement.\n",
    "\n",
    "To agree, simply type `I agree to the Terms of Use` in the next code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I agree to the Terms of Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. A simple 3D CNN with pytorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will create a very simple 3D Convolutional Neural Network on some toy data. This is meant as a quick introduction into the pytorch framework if you haven't used it yet. It will cover everything you need to know for the following parts of the exercise; if you want to go into a bit more detail about the framework, a good place to start is the official [pytorch quickstart tutorial](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html).\n",
    "\n",
    "A very useful resource if you want to know more about a certain class or function are the [official docs](https://pytorch.org/docs/stable/index.html). Take a minute to look at the documentation of a [Tensor](https://pytorch.org/docs/stable/tensors.html?highlight=tensor#torch.Tensor) which is the data structure pytorch uses throughout their APIs. It is comparable to a numpy array, with added functionality such as the ability to move seamlessly between cpu host memory and cuda device memory or the integrated autograd functionality for automatic differentiation. Most importantly, you can create a tensor from a numpy array with `torch.from_numpy(array)` and the other way around with `array = tensor.numpy()`.\n",
    "\n",
    "Our objective with this 3D CNN is to simply classify if a given SDF is containing a sphere or a torus. The data is generated the same way we did it in the last exercise (we randomize the shape parameters a bit so the task is not too trivial). Each such sample has a scalar label associated with it, either 0 or 1, depending on the shape it contains.\n",
    "\n",
    "All of the implementation in this part will take place in `exercise_2/simple_nn.py`. This makes sense for now but note that we would usually spread things over multiple different files like in the following exercise parts.\n",
    "\n",
    "The important bits we need for every training in pytorch are:\n",
    "1. The **dataset** implementation. This is a class responsible for providing the data used by the training procedure sample-by-sample. In its simplest form, it just loads data samples (eg. point clouds or voxel grids) from disk. However, it is usually also used to transform raw data from disk into the correct format and to apply various kinds of augmentations.\n",
    "2. The network definition (\"**model**\") that we want to train. It specifies the network structure (number and type of layers, activation functions, normalization layers etc.) as well as how exactly input data is processed.\n",
    "3. The **loss function**: For classification, this is usually a Cross Entropy Loss; we will also see reconstruction losses like l1 and l2 in exercise 3.\n",
    "4. The **optimizer**: Usually chosen from a set of pre-defined classes like SGD or ADAM.\n",
    "5. The **training loop**: For each batch of data, it does the following: Loads data from the dataloader, passes it through the model (\"forward pass\"), computes the loss, calculates the gradients (\"backward pass\"), and finally, adjusts the network weights using the optimizer.\n",
    "\n",
    "Let's walk through everything step-by-step:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Dataset\n",
    "\n",
    "We start by implementing the data source, which is called `Dataset` in pytorch. Take a look at the `SimpleDataset` class, it contains everything we have to implement to make it work:\n",
    "1. The `__init__` function that takes paramters and prepares the dataset by setting paths etc. Here, the one thing you always have to do is to load at least a list of samples you want to use - they don't have to be loaded from disk yet, but you need to know which and how many samples there are. This usually also depends on the current split - you use a different set of samples for training than for validation or testing.\n",
    "2. The `__len__` function that returns the total number of samples. In most cases, this will simply be the length of the list of samples you prepared in `__init__`.\n",
    "3. The `__getitem__` function. This is where the actual dataloading takes place. The function gets an index between 0 and the length you returned with `__len__`. Your job is then to return some data corresponding to the index. You do not need to worry about putting data onto the GPU here yet; instead, you load the data and return a tuple containing numpy arrays and other data like sample ids you might need in your training loop. Overall, you should take the index parameter in this function, find the sample id it belongs to, load that data, and then return it in a format that can be used in the training loop.\n",
    "\n",
    "\n",
    "\n",
    "Note that it is also fine to load all data into a list in the `__init__` function for small datasets like our toy dataset in this exercise part. You would then just index this pre-loaded data in your `__getitem__` function.\n",
    "\n",
    "Add and implement functions `__init__`, `__len__`, and `__getitem__` in class `SimpleDataset`. \n",
    "\n",
    "First, implement `__init__`: It takes a single parameter (called `split`) that determines if we are using train or val split at the moment. Based on that, it should generate toy data: 4096 samples if the split is train and 1024 if it is val. Use `generate_toy_data` from `exercise_2/util/toy_data.py` for this which generates two numpy arrays: The first one containing sphere and torus SDFs generated with randomized parameters and the second one classification labels, a 0 if the corresponding SDF in the first array is a sphere and 1 if it is a torus.\n",
    "\n",
    "Then, implement `__len__` by simply returning the length of the data you generated in `__init__`.\n",
    "\n",
    "Lastly, implement `__getitem__` which takes an integer index and returns a tuple: A numpy array containing the input SDF volume as generated by `generate_toy_data` before and a scalar value that represents the target class label for this volume.\n",
    "\n",
    "Note: You have to add an additional dimension of size 1 to the input volume you return in `__getitem__` to make everything work later on, i.e. instead of returning a numpy array of shape (32, 32, 32), you should return an array of shape (1, 32, 32, 32).\n",
    "\n",
    "Test your implementation with the checks below:\n",
    "\n",
    "It is also recommended to check out [pytorch data tutorial](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) to have better understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating toy data ...\n",
      "Generating toy data ...\n",
      "Length of train set: 4096\n",
      "Length of val set: 1024\n",
      "(1, 32, 32, 32)\n",
      "Class = 0\n",
      "(1, 32, 32, 32)\n",
      "Class = 1\n"
     ]
    }
   ],
   "source": [
    "from exercise_2.simple_nn import SimpleDataset\n",
    "\n",
    "# Create datasets with train and val splits\n",
    "train_dataset = SimpleDataset('train')\n",
    "val_dataset = SimpleDataset('val')\n",
    "\n",
    "# Test lengths\n",
    "print(f'Length of train set: {len(train_dataset)}')  # expected output: 4096\n",
    "print(f'Length of val set: {len(val_dataset)}')  # expected output: 1024\n",
    "\n",
    "# Get sample at index 0\n",
    "train_sample = train_dataset[0]\n",
    "print(train_sample[0].shape)  # Expected output (1, 32, 32, 32) (the leading 1 is important for later)\n",
    "print(f\"Class = {train_sample[1]}\")  # Expected output: Scalar value 0\n",
    "val_sample = val_dataset[-1]\n",
    "print(val_sample[0].shape)  # Expected output (1, 32, 32, 32) (the leading 1 is important for later)\n",
    "print(f\"Class = {val_sample[1]}\")  # Expected output: Scalar value 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SDF visualization for 32^3 grid ...\n",
      "Exported to /home/saroyr/tum/ML43D/exercise2/toy_data.ply\n"
     ]
    }
   ],
   "source": [
    "from exercise_2.util.visualization import visualize_sdf\n",
    "\n",
    "train_sample = train_dataset[0]\n",
    "visualize_sdf(train_sample[0].squeeze(0), filename=Path.cwd() / 'toy_data.ply')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Model\n",
    "The model is defined in class `SimpleModel`. Two functions are important here: `__init__` which sets up the architecture by instantiating layers with the correct parameters and adapting them to possible input parameters and `forward` which takes an input tensor (we call it `x` here). The output of `forward` is another tensor that contains the result of the network - in our case, this is a vector of logits which will then be used for classification. It could also be a volume of labels if you do segmentation or a volume of the same size as the input if you do reconstruction.\n",
    "\n",
    "Analogous to `forward`, you could also define a custom `backward` function that describes how to calculate the gradients for your model. However, this is not necessary for most architectures as pytorch's autograd implementation takes care of all of that.\n",
    "\n",
    "For now, we will implement a very simple model: We stack together three 3D Convolution layers and have a fully connected layer at the end for classification. This is a schematic for the overall model structure:\n",
    "<img src=\"exercise_2/images/simplenn.png\" alt=\"simplenn_architecture\" style=\"width: 512px;\"/>\n",
    "\n",
    "Each layer starts with a 3d convolution, followed by a batch normalization layer and a ReLU activation function. We left the implementation of the first layer in there; your task is to fill out the missing parts in `__init__` for layers 2 and 3 (use the same parameters as for the first convolution: kernel size 4, stride 3, padding 1). Each convolution should double the number of feature channels, such that you get a volume of size 16 x 1 x 1 x 1 after layer 3. You also need to define the ReLU and the fully connected layer (for this, use `torch.nn.Linear` to reduce the number of dimensions from 16 to 2).\n",
    "\n",
    "In `forward`, you can repeat the first line to move the input tensor through all three convolutional layers. Then, reshape the resulting tensor to dimension batchsize x 16 (the batch size is always the first dimension) and apply the linear layer to it. Return the result as-is.\n",
    "\n",
    "Hint: The ease of debugging is one of the main reasons why pytorch is preferred in research over alternatives like tensorflow. You can set breakpoints in the `forward` function and watch as the input tensor gets modified by each layer to find bugs or things like wrongly calculated convolution parameters.\n",
    "\n",
    "Use the following sanity checks to verify your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  | Name  | Type        | Params\n",
      "--------------------------------------\n",
      "0 | conv1 | Conv3d      | 260   \n",
      "1 | bn1   | BatchNorm3d | 8     \n",
      "2 | conv2 | Conv3d      | 2056  \n",
      "3 | bn2   | BatchNorm3d | 16    \n",
      "4 | conv3 | Conv3d      | 8208  \n",
      "5 | bn3   | BatchNorm3d | 32    \n",
      "6 | ffn   | Linear      | 34    \n",
      "7 | relu  | ReLU        | 0     \n",
      "8 | TOTAL | SimpleModel | 10614 \n",
      "Output tensor shape:  torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "from exercise_2.simple_nn import SimpleModel\n",
    "from exercise_2.util.model import summarize_model\n",
    "\n",
    "simple_nn = SimpleModel()\n",
    "print(summarize_model(simple_nn))  # Expected: Rows 0-8 and TOTAL = 10614\n",
    "\n",
    "input_tensor = torch.randn(32, 1, 32, 32, 32)\n",
    "predictions = simple_nn(input_tensor)\n",
    "\n",
    "print('Output tensor shape: ', predictions.shape)  # Expected: torch.Size([32, 2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already laid out most of the code structure you need to start training once you have your dataset and model defined. This code usually does not change much between projects; this is why there exist many different libraries to simplify it even more (you can take a look at pytorch lightning for example if you are interested). We will stick to the standard pytorch way of doing things for these exercises.\n",
    "\n",
    "The most important things left to do are:\n",
    "1. Define the data loaders. They take the samples you provide in your dataset implementation and take care of loading multiple samples in parallel, shuffling the dataset, and combining samples into batches.\n",
    "2. Define a loss function. For classification, this is usually `torch.nn.CrossEntropyLoss`\n",
    "3. Instantiate the optimizer. Mostly, this will be ADAM (`torch.optim.Adam`) unless you have a good reason to use another one.\n",
    "4. Implement the training loop: Get a batch of data from the data loader, move it to the GPU, perform a forward pass, compute the loss, calculate gradients in the backward pass, adjust weights in an optimizer step, repeat.\n",
    "\n",
    "Note: You have to move some stuff to the correct compute device (usually the GPU) by calling `.to(device)`: The model and loss function as well as the data you get from each batch.\n",
    "\n",
    "Take a look at the structure of `main` since you will also use it for the next exercise parts and fill in code in `train` at the blanks marked with TODO. Then, start the training below. For this exercise, we don't care too much about the results since it is a very easy classification task anyways. Your model should be able to get to > 98% validation accuracy, though. You can stop the training once that is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Generating toy data ...\n",
      "Generating toy data ...\n",
      "[000/00024] train_loss: 0.476\n",
      "[000/00049] train_loss: 0.304\n",
      "[000/00074] train_loss: 0.270\n",
      "[000/00099] train_loss: 0.254\n",
      "[000/00099] val_loss: 0.202, val_accuracy: 94.727%\n",
      "[000/00124] train_loss: 0.214\n",
      "[001/00021] train_loss: 0.159\n",
      "[001/00046] train_loss: 0.153\n",
      "[001/00071] train_loss: 0.156\n",
      "[001/00071] val_loss: 0.128, val_accuracy: 97.168%\n",
      "[001/00096] train_loss: 0.148\n",
      "[001/00121] train_loss: 0.114\n",
      "[002/00018] train_loss: 0.079\n",
      "[002/00043] train_loss: 0.138\n",
      "[002/00043] val_loss: 0.120, val_accuracy: 98.145%\n",
      "[002/00068] train_loss: 0.132\n",
      "[002/00093] train_loss: 0.133\n",
      "[002/00118] train_loss: 0.114\n",
      "[003/00015] train_loss: 0.057\n",
      "[003/00015] val_loss: 0.081, val_accuracy: 97.949%\n",
      "[003/00040] train_loss: 0.089\n",
      "[003/00065] train_loss: 0.097\n",
      "[003/00090] train_loss: 0.093\n",
      "[003/00115] train_loss: 0.106\n",
      "[003/00115] val_loss: 0.075, val_accuracy: 97.852%\n",
      "[004/00012] train_loss: 0.039\n",
      "[004/00037] train_loss: 0.084\n",
      "[004/00062] train_loss: 0.073\n",
      "[004/00087] train_loss: 0.080\n",
      "[004/00087] val_loss: 0.056, val_accuracy: 98.926%\n",
      "[004/00112] train_loss: 0.093\n",
      "[005/00009] train_loss: 0.027\n",
      "[005/00034] train_loss: 0.091\n",
      "[005/00059] train_loss: 0.090\n",
      "[005/00059] val_loss: 0.091, val_accuracy: 97.461%\n",
      "[005/00084] train_loss: 0.091\n",
      "[005/00109] train_loss: 0.105\n",
      "[006/00006] train_loss: 0.018\n",
      "[006/00031] train_loss: 0.070\n",
      "[006/00031] val_loss: 0.059, val_accuracy: 98.438%\n",
      "[006/00056] train_loss: 0.080\n",
      "[006/00081] train_loss: 0.072\n",
      "[006/00106] train_loss: 0.045\n",
      "[007/00003] train_loss: 0.008\n",
      "[007/00003] val_loss: 0.048, val_accuracy: 98.730%\n",
      "[007/00028] train_loss: 0.098\n",
      "[007/00053] train_loss: 0.077\n",
      "[007/00078] train_loss: 0.043\n",
      "[007/00103] train_loss: 0.045\n",
      "[007/00103] val_loss: 0.071, val_accuracy: 98.145%\n",
      "[008/00000] train_loss: 0.000\n",
      "[008/00025] train_loss: 0.045\n",
      "[008/00050] train_loss: 0.067\n",
      "[008/00075] train_loss: 0.041\n",
      "[008/00075] val_loss: 0.057, val_accuracy: 98.340%\n",
      "[008/00100] train_loss: 0.060\n",
      "[008/00125] train_loss: 0.074\n",
      "[009/00022] train_loss: 0.032\n",
      "[009/00047] train_loss: 0.043\n",
      "[009/00047] val_loss: 0.039, val_accuracy: 98.926%\n",
      "[009/00072] train_loss: 0.073\n",
      "[009/00097] train_loss: 0.042\n",
      "[009/00122] train_loss: 0.071\n",
      "[010/00019] train_loss: 0.049\n",
      "[010/00019] val_loss: 0.084, val_accuracy: 98.438%\n",
      "[010/00044] train_loss: 0.036\n",
      "[010/00069] train_loss: 0.031\n",
      "[010/00094] train_loss: 0.071\n",
      "[010/00119] train_loss: 0.045\n",
      "[010/00119] val_loss: 0.044, val_accuracy: 98.730%\n",
      "[011/00016] train_loss: 0.027\n",
      "[011/00041] train_loss: 0.032\n",
      "[011/00066] train_loss: 0.049\n",
      "[011/00091] train_loss: 0.073\n",
      "[011/00091] val_loss: 0.047, val_accuracy: 98.926%\n",
      "[011/00116] train_loss: 0.028\n",
      "[012/00013] train_loss: 0.048\n",
      "[012/00038] train_loss: 0.122\n",
      "[012/00063] train_loss: 0.064\n",
      "[012/00063] val_loss: 0.049, val_accuracy: 98.828%\n",
      "[012/00088] train_loss: 0.056\n",
      "[012/00113] train_loss: 0.053\n",
      "[013/00010] train_loss: 0.019\n",
      "[013/00035] train_loss: 0.040\n",
      "[013/00035] val_loss: 0.059, val_accuracy: 98.730%\n",
      "[013/00060] train_loss: 0.082\n",
      "[013/00085] train_loss: 0.043\n",
      "[013/00110] train_loss: 0.050\n",
      "[014/00007] train_loss: 0.005\n",
      "[014/00007] val_loss: 0.044, val_accuracy: 99.023%\n",
      "[014/00032] train_loss: 0.021\n",
      "[014/00057] train_loss: 0.043\n",
      "[014/00082] train_loss: 0.041\n",
      "[014/00107] train_loss: 0.043\n",
      "[014/00107] val_loss: 0.032, val_accuracy: 99.219%\n",
      "[015/00004] train_loss: 0.006\n",
      "[015/00029] train_loss: 0.027\n",
      "[015/00054] train_loss: 0.038\n",
      "[015/00079] train_loss: 0.077\n",
      "[015/00079] val_loss: 0.091, val_accuracy: 97.461%\n",
      "[015/00104] train_loss: 0.065\n",
      "[016/00001] train_loss: 0.006\n",
      "[016/00026] train_loss: 0.056\n",
      "[016/00051] train_loss: 0.038\n",
      "[016/00051] val_loss: 0.053, val_accuracy: 98.926%\n",
      "[016/00076] train_loss: 0.055\n",
      "[016/00101] train_loss: 0.031\n",
      "[016/00126] train_loss: 0.047\n",
      "[017/00023] train_loss: 0.027\n",
      "[017/00023] val_loss: 0.069, val_accuracy: 98.438%\n",
      "[017/00048] train_loss: 0.082\n",
      "[017/00073] train_loss: 0.049\n",
      "[017/00098] train_loss: 0.063\n",
      "[017/00123] train_loss: 0.069\n",
      "[017/00123] val_loss: 0.047, val_accuracy: 98.730%\n",
      "[018/00020] train_loss: 0.057\n",
      "[018/00045] train_loss: 0.081\n",
      "[018/00070] train_loss: 0.054\n",
      "[018/00095] train_loss: 0.046\n",
      "[018/00095] val_loss: 0.037, val_accuracy: 98.926%\n",
      "[018/00120] train_loss: 0.052\n",
      "[019/00017] train_loss: 0.012\n",
      "[019/00042] train_loss: 0.053\n",
      "[019/00067] train_loss: 0.050\n",
      "[019/00067] val_loss: 0.055, val_accuracy: 98.438%\n",
      "[019/00092] train_loss: 0.042\n",
      "[019/00117] train_loss: 0.037\n",
      "[020/00014] train_loss: 0.019\n",
      "[020/00039] train_loss: 0.053\n",
      "[020/00039] val_loss: 0.054, val_accuracy: 98.535%\n",
      "[020/00064] train_loss: 0.037\n",
      "[020/00089] train_loss: 0.046\n",
      "[020/00114] train_loss: 0.053\n",
      "[021/00011] train_loss: 0.005\n",
      "[021/00011] val_loss: 0.034, val_accuracy: 99.023%\n",
      "[021/00036] train_loss: 0.045\n",
      "[021/00061] train_loss: 0.038\n",
      "[021/00086] train_loss: 0.039\n",
      "[021/00111] train_loss: 0.043\n",
      "[021/00111] val_loss: 0.067, val_accuracy: 98.535%\n",
      "[022/00008] train_loss: 0.010\n",
      "[022/00033] train_loss: 0.048\n",
      "[022/00058] train_loss: 0.035\n",
      "[022/00083] train_loss: 0.066\n",
      "[022/00083] val_loss: 0.044, val_accuracy: 98.926%\n",
      "[022/00108] train_loss: 0.025\n",
      "[023/00005] train_loss: 0.008\n",
      "[023/00030] train_loss: 0.032\n",
      "[023/00055] train_loss: 0.065\n",
      "[023/00055] val_loss: 0.059, val_accuracy: 98.730%\n",
      "[023/00080] train_loss: 0.046\n",
      "[023/00105] train_loss: 0.050\n",
      "[024/00002] train_loss: 0.003\n",
      "[024/00027] train_loss: 0.020\n",
      "[024/00027] val_loss: 0.034, val_accuracy: 99.023%\n",
      "[024/00052] train_loss: 0.034\n",
      "[024/00077] train_loss: 0.057\n",
      "[024/00102] train_loss: 0.068\n",
      "[024/00127] train_loss: 0.039\n",
      "[024/00127] val_loss: 0.035, val_accuracy: 99.023%\n"
     ]
    }
   ],
   "source": [
    "from exercise_2 import simple_nn\n",
    "\n",
    "config={\n",
    "    'experiment_name': '2_2_simple_nn',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'batch_size': 32,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate': 0.001,\n",
    "    'max_epochs': 25,\n",
    "    'print_every_n': 25,\n",
    "    'validate_every_n': 100\n",
    "}\n",
    "\n",
    "simple_nn.main(config)  # should have val_accuracy > 99%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! In the following parts, we will now move on to more complicated problems and more involved network models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Shape Classification using 3DCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Download and extract voxelized ShapeNet training data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each folder in the `exercise_2/data/ShapeNetVox32` directory represents a shape category represented by a number, e.g. `02691156`.\n",
    "We provide the mapping between these numbers and the corresponding names in `exercise_2/data/shape_info.json`. Each of these shape category folders contains a number of shapes\n",
    "represented as voxels and stored in a `binvox` format.\n",
    "\n",
    "```\n",
    "# contents of exercise_2/data/ShapeNetVox32\n",
    "\n",
    "02691156/                                   # Shape category folder with all it's shapes\n",
    "    ├── 1a04e3eab45ca15dd86060f189eb133/    # A single shape of the category\n",
    "        ├── model.binvox                    # Voxel representation of the shape in binvox format\n",
    "    ├── 1a6ad7a24bb89733f412783097373bdc/   # Another shape of the category\n",
    "    ├── 1a9b552befd6306cc8f2d5fe7449af61/\n",
    "    ├── :                                   # And so on ...\n",
    "    ├── :\n",
    "02828884/                                   # Another shape category folder\n",
    "02933112/                                   # In total you should have 13 shape category folders\n",
    ":\n",
    ":\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ...\n",
      "--2025-11-20 07:40:52--  http://cvgl.stanford.edu/data2/ShapeNetVox32.tgz\n",
      "Resolving cvgl.stanford.edu (cvgl.stanford.edu)... 171.64.64.64\n",
      "Connecting to cvgl.stanford.edu (cvgl.stanford.edu)|171.64.64.64|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://cvgl.stanford.edu/data2/ShapeNetVox32.tgz [following]\n",
      "--2025-11-20 07:40:52--  https://cvgl.stanford.edu/data2/ShapeNetVox32.tgz\n",
      "Connecting to cvgl.stanford.edu (cvgl.stanford.edu)|171.64.64.64|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 22218020 (21M) [application/x-gzip]\n",
      "Saving to: ‘exercise_2/data/ShapeNetVox32.tgz’\n",
      "\n",
      "ShapeNetVox32.tgz   100%[===================>]  21.19M  1.98MB/s    in 18s     \n",
      "\n",
      "2025-11-20 07:41:11 (1.18 MB/s) - ‘exercise_2/data/ShapeNetVox32.tgz’ saved [22218020/22218020]\n",
      "\n",
      "Extracting ...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print('Downloading ...')\n",
    "!wget http://cvgl.stanford.edu/data2/ShapeNetVox32.tgz -P exercise_2/data\n",
    "print('Extracting ...')\n",
    "!tar -xzf exercise_2/data/ShapeNetVox32.tgz -C exercise_2/data\n",
    "!rm exercise_2/data/ShapeNetVox32.tgz\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Dataloading and exploring the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We already provide you with a training and validation split in files `train.txt` and `val.txt` in folder `exercise_2/data/splits/shapenet`.\n",
    "All the shapes in the list `train.txt` make up the training samples, while all the samples in `val.txt` constitute the validation set.\n",
    "Additionally, we provide `overfit.txt` as the set of shapes we'll use for overfitting / debugging later.\n",
    "\n",
    "Now let's write a Pytorch Dataset class that can load this data from the disk. Check out `ShapeNetVox` class in file `exercise_2/data/shapenet.py`\n",
    "for a partial implementation of such a dataset.\n",
    "\n",
    "The dataset class is instantiated with the type of the split, e.g. `train`, `val` or `overfit` and loads all\n",
    "shape names for that split as a list in its member variable `self.items`. The class also provides utility method `get_shape_voxels(shapenet_id)`\n",
    "which given a `shapenet_id` of the form `<shape_class>/<shape_identifier>` returns a 32x32x32 numpy array representing the voxels of the shape.\n",
    "The class also provides a list of all shape categories as the static member `ShapeNetVox.classes`.\n",
    "\n",
    "Your task is to fill out the missing implementations of functions `__getitem__` and `__len__` as specified by their docstrings.\n",
    "Once done, test your implementation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_2.data.shapenet import ShapeNetVox\n",
    "# Let's test your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train set: 21705\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset with train split\n",
    "trainset = ShapeNetVox('train')\n",
    "\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of train set: {len(trainset)}')  # expected output: 21705"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of validation set: 5426\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset with val split and print its length\n",
    "valset = ShapeNetVox('val')\n",
    "print(f'Length of validation set: {len(valset)}')  # expected output: 5426"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize some shapes\n",
    "from exercise_2.util.visualization import visualize_occupancy, visualize_pointcloud\n",
    "\n",
    "shape_data = trainset[0]\n",
    "print(f'Name: {shape_data[\"name\"]}')  # expected output: 04379243/d120d47f8c9bc5028640bc5712201c4a\n",
    "print(f'Voxel Dimensions: {shape_data[\"voxel\"].shape}')  # expected output: (1, 32, 32, 32)\n",
    "print(f'Label: {shape_data[\"label\"]} | {ShapeNetVox.classes[shape_data[\"label\"]]}')  # expected output: 10, 04379243\n",
    "\n",
    "def visualize_occupancy(occupancy_grid, flip_axes=False):\n",
    "    point_list = np.concatenate([c[:, np.newaxis] for c in np.where(occupancy_grid)], axis=1)\n",
    "    visualize_pointcloud(point_list, 20, flip_axes=flip_axes, name='occupancy_grid')\n",
    "\n",
    "visualize_occupancy(shape_data[\"voxel\"].squeeze(), flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 03211117/edfc3a8ecc5b07e9feb0fb1dff94c98a\n",
      "Voxel Dimensions: (1, 32, 32, 32)\n",
      "Label: 5 | 03211117\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53ef163f109942998686545ad19329a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shape_data = trainset[7]\n",
    "print(f'Name: {shape_data[\"name\"]}')  # expected output: 03211117/edfc3a8ecc5b07e9feb0fb1dff94c98a\n",
    "print(f'Voxel Dimensions: {shape_data[\"voxel\"].shape}')  # expected output: (1, 32, 32, 32)\n",
    "print(f'Label: {shape_data[\"label\"]} | {ShapeNetVox.classes[shape_data[\"label\"]]}')  # expected output: 5, 03211117\n",
    "\n",
    "visualize_occupancy(shape_data[\"voxel\"].squeeze(), flip_axes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Defining the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We'll be using the 3DCNN model introduced by Qi et al.[1] to classify shapes. In particular, we'll be using\n",
    "the \"Auxiliary Training by Subvolume Supervision\" model presented in `Section 4.2` / `Fig. 3` of the paper.\n",
    "\n",
    "<img src=\"exercise_2/images/3dcnn.png\" alt=\"3dcnn_architecture\" style=\"width: 1024px;\"/>\n",
    "\n",
    "Fill in the model implementation of `MLPConv` and `ThreeDeeCNN`(which would use `MLPConv` layers) in the file `exercise_2/model/cnn3d.py`.\n",
    "Since the 3DCNN model predicts 9 labels for each samples (1 global + 8 local), we expect the output tensor to be of the shape `(B, 9, N_cls)` where `B` is the batch size, `N_cls` are number of classes (13 in our case).\n",
    "This means that the global and auxilary parts of the network output a score per class.\n",
    "\n",
    "Here are some basic sanity tests for your implemetation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   | Name                 | Type        | Params  \n",
      "--------------------------------------------------------\n",
      "0  | backbone             | Sequential  | 3764464 \n",
      "1  | backbone.0           | MLPConv     | 15120   \n",
      "2  | backbone.0.model     | Sequential  | 15120   \n",
      "3  | backbone.0.model.0   | Conv3d      | 10416   \n",
      "4  | backbone.0.model.1   | ReLU        | 0       \n",
      "5  | backbone.0.model.2   | Conv3d      | 2352    \n",
      "6  | backbone.0.model.3   | ReLU        | 0       \n",
      "7  | backbone.0.model.4   | Conv3d      | 2352    \n",
      "8  | backbone.0.model.5   | ReLU        | 0       \n",
      "9  | backbone.1           | MLPConv     | 1011680 \n",
      "10 | backbone.1.model     | Sequential  | 1011680 \n",
      "11 | backbone.1.model.0   | Conv3d      | 960160  \n",
      "12 | backbone.1.model.1   | ReLU        | 0       \n",
      "13 | backbone.1.model.2   | Conv3d      | 25760   \n",
      "14 | backbone.1.model.3   | ReLU        | 0       \n",
      "15 | backbone.1.model.4   | Conv3d      | 25760   \n",
      "16 | backbone.1.model.5   | ReLU        | 0       \n",
      "17 | backbone.2           | MLPConv     | 2737664 \n",
      "18 | backbone.2.model     | Sequential  | 2737664 \n",
      "19 | backbone.2.model.0   | Conv3d      | 2212352 \n",
      "20 | backbone.2.model.1   | ReLU        | 0       \n",
      "21 | backbone.2.model.2   | Conv3d      | 262656  \n",
      "22 | backbone.2.model.3   | ReLU        | 0       \n",
      "23 | backbone.2.model.4   | Conv3d      | 262656  \n",
      "24 | backbone.2.model.5   | ReLU        | 0       \n",
      "25 | partial_predictors   | ModuleList  | 53352   \n",
      "26 | partial_predictors.0 | Linear      | 6669    \n",
      "27 | partial_predictors.1 | Linear      | 6669    \n",
      "28 | partial_predictors.2 | Linear      | 6669    \n",
      "29 | partial_predictors.3 | Linear      | 6669    \n",
      "30 | partial_predictors.4 | Linear      | 6669    \n",
      "31 | partial_predictors.5 | Linear      | 6669    \n",
      "32 | partial_predictors.6 | Linear      | 6669    \n",
      "33 | partial_predictors.7 | Linear      | 6669    \n",
      "34 | full_predictor       | Sequential  | 12613645\n",
      "35 | full_predictor.0     | Linear      | 8390656 \n",
      "36 | full_predictor.1     | ReLU        | 0       \n",
      "37 | full_predictor.2     | Linear      | 4196352 \n",
      "38 | full_predictor.3     | ReLU        | 0       \n",
      "39 | full_predictor.4     | Linear      | 26637   \n",
      "40 | TOTAL                | ThreeDeeCNN | 16431461\n",
      "Output tensor shape:  torch.Size([8, 9, 13])\n",
      "Number of traininable params: 16.43M\n"
     ]
    }
   ],
   "source": [
    "from exercise_2.model.cnn3d import ThreeDeeCNN\n",
    "from exercise_2.util.model import summarize_model\n",
    "\n",
    "cnn3d = ThreeDeeCNN(13)\n",
    "print(summarize_model(cnn3d))  # Expected: Rows 0-40 and TOTAL = 16431461\n",
    "\n",
    "input_tensor = torch.randn(8, 1, 32, 32, 32)\n",
    "predictions = cnn3d(input_tensor)\n",
    "\n",
    "print('Output tensor shape: ', predictions.shape)  # expected output: 8, 9, 13\n",
    "\n",
    "num_trainable_params = sum(p.numel() for p in cnn3d.parameters() if p.requires_grad) / 1e6\n",
    "print(f'Number of traininable params: {num_trainable_params:.2f}M')  # expected output: ~16M"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Training Script\n",
    "\n",
    "Now that we have the dataset class and the model class, we just need a training script\n",
    "that trains the model using data from train dataset and evaluates the model's validation\n",
    "performance during training. A partial script is provided in `exercise_2/training/train_3dcnn.py`.\n",
    "Fill in the missing blocks to make the training work."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Overfitting to a few samples\n",
    "Before training on entire data, it is usually a good idea to try training on a small subset of data,\n",
    "to see if your model can do forward and backward passes without any errors, your metrics work, and that you model can\n",
    "overfit with a very low error on this small set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexercise_2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtraining\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_3dcnn\n\u001b[32m      2\u001b[39m config = {\n\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mexperiment_name\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33m2_3_3dcnn_overfitting\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mcuda:0\u001b[39m\u001b[33m'\u001b[39m,                      \u001b[38;5;66;03m# change this to cpu if you do not have a GPU\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mvalidate_every_n\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m100\u001b[39m,\n\u001b[32m     12\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mtrain_3dcnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# should be able to get ~0 loss, 100% accuracy\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tum/ML43D/exercise2/exercise_2/training/train_3dcnn.py:56\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(config)\u001b[39m\n\u001b[32m     53\u001b[39m Path(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mexercise_2/runs/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[33m\"\u001b[39m\u001b[33mexperiment_name\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m).mkdir(exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m, parents=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# start training\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/tum/ML43D/exercise2/exercise_2/training/train_3dcnn.py:100\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, trainloader, valloader, device, config)\u001b[39m\n\u001b[32m     97\u001b[39m optimizer.step()\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# loss logging\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m train_loss_running += \u001b[43mloss_total\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m iteration = epoch * \u001b[38;5;28mlen\u001b[39m(trainloader) + i\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m iteration % config[\u001b[33m'\u001b[39m\u001b[33mprint_every_n\u001b[39m\u001b[33m'\u001b[39m] == (config[\u001b[33m'\u001b[39m\u001b[33mprint_every_n\u001b[39m\u001b[33m'\u001b[39m] - \u001b[32m1\u001b[39m):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from exercise_2.training import train_3dcnn\n",
    "config = {\n",
    "    'experiment_name': '2_3_3dcnn_overfitting',\n",
    "    'device': 'cuda:0',                      # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': True,                      # True since we're doing overfitting\n",
    "    'batch_size': 16,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate': 0.0005,\n",
    "    'max_epochs': 300,\n",
    "    'print_every_n': 100,\n",
    "    'validate_every_n': 100,\n",
    "}\n",
    "\n",
    "train_3dcnn.main(config)  # should be able to get ~0 loss, 100% accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### (f) Training over the entire training set\n",
    "If the overfitting works, we can go ahead with training on the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "[000/00099] train_loss: 20.230\n",
      "[000/00199] train_loss: 14.788\n",
      "[000/00249] val_loss: 9.478, val_accuracy: 71.065%\n",
      "[000/00299] train_loss: 9.510\n",
      "[000/00399] train_loss: 7.158\n",
      "[000/00499] train_loss: 5.555\n",
      "[000/00499] val_loss: 5.844, val_accuracy: 78.787%\n",
      "[000/00599] train_loss: 5.426\n",
      "[000/00699] train_loss: 4.882\n",
      "[000/00749] val_loss: 5.099, val_accuracy: 84.998%\n",
      "[000/00799] train_loss: 4.647\n",
      "[000/00899] train_loss: 4.409\n",
      "[000/00999] train_loss: 4.355\n",
      "[000/00999] val_loss: 4.360, val_accuracy: 86.344%\n",
      "[000/01099] train_loss: 4.743\n",
      "[000/01199] train_loss: 4.273\n",
      "[000/01249] val_loss: 4.065, val_accuracy: 88.537%\n",
      "[000/01299] train_loss: 4.167\n",
      "[001/00042] train_loss: 3.800\n",
      "[001/00142] train_loss: 3.722\n",
      "[001/00142] val_loss: 3.778, val_accuracy: 89.200%\n",
      "[001/00242] train_loss: 3.526\n",
      "[001/00342] train_loss: 3.601\n",
      "[001/00392] val_loss: 3.685, val_accuracy: 89.956%\n",
      "[001/00442] train_loss: 3.702\n",
      "[001/00542] train_loss: 3.442\n",
      "[001/00642] train_loss: 3.717\n",
      "[001/00642] val_loss: 3.490, val_accuracy: 89.900%\n",
      "[001/00742] train_loss: 3.519\n",
      "[001/00842] train_loss: 2.946\n",
      "[001/00892] val_loss: 3.471, val_accuracy: 88.832%\n",
      "[001/00942] train_loss: 3.434\n",
      "[001/01042] train_loss: 3.602\n",
      "[001/01142] train_loss: 3.164\n",
      "[001/01142] val_loss: 3.583, val_accuracy: 89.513%\n",
      "[001/01242] train_loss: 3.182\n",
      "[001/01342] train_loss: 3.324\n",
      "[002/00035] val_loss: 3.451, val_accuracy: 89.440%\n",
      "[002/00085] train_loss: 2.964\n",
      "[002/00185] train_loss: 3.166\n",
      "[002/00285] train_loss: 3.020\n",
      "[002/00285] val_loss: 3.310, val_accuracy: 91.098%\n",
      "[002/00385] train_loss: 2.986\n",
      "[002/00485] train_loss: 3.043\n",
      "[002/00535] val_loss: 3.058, val_accuracy: 91.725%\n",
      "[002/00585] train_loss: 2.973\n",
      "[002/00685] train_loss: 3.139\n",
      "[002/00785] train_loss: 2.878\n",
      "[002/00785] val_loss: 3.096, val_accuracy: 91.541%\n",
      "[002/00885] train_loss: 3.173\n",
      "[002/00985] train_loss: 2.780\n",
      "[002/01035] val_loss: 2.937, val_accuracy: 91.799%\n",
      "[002/01085] train_loss: 2.928\n",
      "[002/01185] train_loss: 2.961\n",
      "[002/01285] train_loss: 2.982\n",
      "[002/01285] val_loss: 3.010, val_accuracy: 91.209%\n",
      "[003/00028] train_loss: 2.543\n",
      "[003/00128] train_loss: 2.863\n",
      "[003/00178] val_loss: 2.941, val_accuracy: 91.854%\n",
      "[003/00228] train_loss: 2.840\n",
      "[003/00328] train_loss: 3.001\n",
      "[003/00428] train_loss: 2.707\n",
      "[003/00428] val_loss: 3.158, val_accuracy: 91.320%\n",
      "[003/00528] train_loss: 2.825\n",
      "[003/00628] train_loss: 2.456\n",
      "[003/00678] val_loss: 3.176, val_accuracy: 91.651%\n",
      "[003/00728] train_loss: 2.184\n",
      "[003/00828] train_loss: 2.645\n",
      "[003/00928] train_loss: 2.734\n",
      "[003/00928] val_loss: 2.895, val_accuracy: 91.743%\n",
      "[003/01028] train_loss: 2.615\n",
      "[003/01128] train_loss: 2.592\n",
      "[003/01178] val_loss: 2.960, val_accuracy: 90.472%\n",
      "[003/01228] train_loss: 2.696\n",
      "[003/01328] train_loss: 2.621\n",
      "[004/00071] train_loss: 2.493\n",
      "[004/00071] val_loss: 2.750, val_accuracy: 92.776%\n",
      "[004/00171] train_loss: 2.265\n",
      "[004/00271] train_loss: 2.531\n",
      "[004/00321] val_loss: 2.722, val_accuracy: 92.333%\n",
      "[004/00371] train_loss: 2.460\n",
      "[004/00471] train_loss: 2.280\n",
      "[004/00571] train_loss: 2.257\n",
      "[004/00571] val_loss: 2.766, val_accuracy: 92.812%\n",
      "[004/00671] train_loss: 2.327\n",
      "[004/00771] train_loss: 2.523\n",
      "[004/00821] val_loss: 2.828, val_accuracy: 92.278%\n",
      "[004/00871] train_loss: 2.480\n",
      "[004/00971] train_loss: 2.462\n",
      "[004/01071] train_loss: 2.334\n",
      "[004/01071] val_loss: 2.684, val_accuracy: 91.504%\n",
      "[004/01171] train_loss: 2.469\n",
      "[004/01271] train_loss: 2.268\n",
      "[004/01321] val_loss: 2.745, val_accuracy: 92.481%\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'experiment_name': '2_3_3dcnn_generalization',\n",
    "    'device': 'cuda:0',                     # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 16,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate': 0.0005,\n",
    "    'max_epochs': 5,\n",
    "    'print_every_n': 100,\n",
    "    'validate_every_n': 250,\n",
    "}\n",
    "\n",
    "train_3dcnn.main(config)                    # should have accuracy > 88% on val set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (g) Inference using the trained model\n",
    "\n",
    "We would now like to make shape category inference given shapes from validation set.\n",
    "Implement the function `infer_single` of class `InferenceHandler3DCNN` in file `exercise_2/inference/infer_3dcnn.py` such that it returns the\n",
    "shape category name predicted by the model given its numpy voxel representation (32x32x32). Note that the network predicts a label in range [0, 12],\n",
    "which can be mapped to a class ID (e.g. 03001627) using `ShapeNetVox.classes` which can further be mapped to a category name (e.g. chair for 03001627) using\n",
    "`ShapeNetVox.class_name_mapping`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_2.inference.infer_3dcnn import InferenceHandler3DCNN\n",
    "\n",
    "# create a handler for inference using a trained checkpoint\n",
    "inferer = InferenceHandler3DCNN('exercise_2/runs/2_3_3dcnn_generalization/model_best.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d7c5fb5201e42fa83216c1ed958dbb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get shape voxels and visualize\n",
    "shape_voxels = ShapeNetVox.get_shape_voxels('03001627/f913501826c588e89753496ba23f2183')\n",
    "visualize_occupancy(shape_voxels, flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category: chair\n"
     ]
    }
   ],
   "source": [
    "# predict category\n",
    "print('Predicted category:', inferer.infer_single(shape_voxels))  # expected output: chair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e13e972a43486c874f44c86e8e9f62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get shape voxels and visualize\n",
    "shape_voxels = ShapeNetVox.get_shape_voxels('02691156/6af4383123972f2262b600da24e0965')\n",
    "visualize_occupancy(shape_voxels, flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category: airplane\n"
     ]
    }
   ],
   "source": [
    "# predict category\n",
    "print('Predicted category:', inferer.infer_single(shape_voxels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3546548a33cd4dd8aeb0614dd8b27b18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get shape voxels and visualize\n",
    "shape_voxels = ShapeNetVox.get_shape_voxels('04090263/eae96ddf483e896c805d3d8e378d155e')\n",
    "visualize_occupancy(shape_voxels, flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category: rifle\n"
     ]
    }
   ],
   "source": [
    "# predict category\n",
    "print('Predicted category:', inferer.infer_single(shape_voxels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you submit the trained model `exercise_2/runs/2_3_3dcnn_generalization/model_best.ckpt` in your zip\n",
    "so that we can evaluate it on the test set at our end."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.4. Shape Classification using PointNet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach you used above works very well when there is voxelized data available. However, many applications require that points are used instead and voxelizing them every time is cumbersome and might lead to unexpected artifacts.\n",
    "\n",
    "This is where PointNet [2] is very useful: It directly takes in a set of points and can perform both classification and semantic segmentation without the need for any gridification or other form of conversion.\n",
    "\n",
    "You already heard a lot about the approach and architecture in the lecture; here, our goal is to create an implementation of it from scratch and try it out on some ShapeNet data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Download and prepare the ShapeNetPointClouds dataset\n",
    "We generated point clouds from ShapeNet meshes via uniform sampling. Each point cloud contains 1024 xyz points.\n",
    "\n",
    "The data layout is basically the same as in 2.3.:\n",
    "Each folder in the `exercise_2/data/ShapeNetPointClouds` directory contains one shape category represented by a number, e.g. `02691156`.\n",
    "We provide the mapping between these numbers and the corresponding names in `exercise_2/data/shape_info.json`. Each of these shape category folders contains a number of shapes in obj format.\n",
    "\n",
    "```\n",
    "# contents of exercise_2/data/ShapeNetPointClouds\n",
    "\n",
    "02691156/                                      # Shape category folder with all its shapes\n",
    "    ├── 1a04e3eab45ca15dd86060f189eb133.obj    # A single shape of the category\n",
    "    ├── 1a6ad7a24bb89733f412783097373bdc.obj   # Another shape of the category\n",
    "    ├── :                                      # And so on ...\n",
    "    ├── :\n",
    "02828884/                                      # Another shape category folder\n",
    "02933112/                                      # In total you should have 13 shape category folders\n",
    ":\n",
    ":\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ...\n",
      "--2025-11-20 17:01:38--  http://kaldir.vc.in.tum.de/cdiller/ShapeNetPointClouds.zip\n",
      "Resolving kaldir.vc.in.tum.de (kaldir.vc.in.tum.de)... 131.159.98.128\n",
      "Connecting to kaldir.vc.in.tum.de (kaldir.vc.in.tum.de)|131.159.98.128|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://kaldir.vc.in.tum.de:443/cdiller/ShapeNetPointClouds.zip [following]\n",
      "--2025-11-20 17:01:38--  https://kaldir.vc.in.tum.de/cdiller/ShapeNetPointClouds.zip\n",
      "Connecting to kaldir.vc.in.tum.de (kaldir.vc.in.tum.de)|131.159.98.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 453765424 (433M) [application/zip]\n",
      "Saving to: ‘exercise_2/data/ShapeNetPointClouds.zip’\n",
      "\n",
      "ShapeNetPointClouds 100%[===================>] 432.74M  5.91MB/s    in 81s     \n",
      "\n",
      "2025-11-20 17:02:59 (5.37 MB/s) - ‘exercise_2/data/ShapeNetPointClouds.zip’ saved [453765424/453765424]\n",
      "\n",
      "Extracting ...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print('Downloading ...')\n",
    "!wget http://kaldir.vc.in.tum.de/cdiller/ShapeNetPointClouds.zip -P exercise_2/data\n",
    "print('Extracting ...')\n",
    "!unzip -q exercise_2/data/ShapeNetPointClouds.zip -d exercise_2/data\n",
    "!rm exercise_2/data/ShapeNetPointClouds.zip\n",
    "print('Done.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Dataset implementation\n",
    "\n",
    "You can use the same split setup as in 2.3: `overfit.txt` for overfitting, `train.txt` for the train samples, and `val.txt` for the val samples in folder `exercise_2/data/splits/shapenet`.\n",
    "\n",
    "The dataset implementation will therefore be very similar to the one from 2.3: Fill out the missing implementations of functions `__getitem__` and `__len__` in class `ShapeNetPoints` in `exercise_2/data/shapenet.py`.\n",
    "\n",
    "The major difference is how the actual data is loaded: We don't have regular voxel grids anymore and instead load arrays of 1024 points each. In `__getitem__`, we now return 'points' instead of 'voxel' and for loading the point clouds, we use `get_point_cloud` instead of `get_shape_voxels`. You can load the point cloud data either by hand (since it is in the same obj format you used in exercise 1) or simply use `trimesh.load`. The point clouds you return from `__getitem__` should have shape 3 x 1024 and datatype `np.float32`.\n",
    "\n",
    "Otherwise, the implementation is very much the same as in 2.3. Once done, test your implementation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train set: 21705\n",
      "Length of val set: 5426\n",
      "Length of overfit set: 64\n"
     ]
    }
   ],
   "source": [
    "from exercise_2.data.shapenet import ShapeNetPoints\n",
    "\n",
    "# Create a dataset with train split\n",
    "train_dataset = ShapeNetPoints('train')\n",
    "val_dataset = ShapeNetPoints('val')\n",
    "overfit_dataset = ShapeNetPoints('overfit')\n",
    "\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of train set: {len(train_dataset)}')  # expected output: 21705\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of val set: {len(val_dataset)}')  # expected output: 5426\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of overfit set: {len(overfit_dataset)}')  # expected output: 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 03001627/d6dd3de6d6ea6d0b1931ace01cf1b948\n",
      "Voxel Dimensions: (3, 1024)\n",
      "Label: 4 | 03001627 | chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "840517cf995f473a98e880ea44b6482e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize some shapes\n",
    "from exercise_2.util.visualization import visualize_pointcloud\n",
    "\n",
    "shape_data = train_dataset[np.random.randint(len(train_dataset))]\n",
    "print(f'Name: {shape_data[\"name\"]}')  # expected output: 04379243/d120d47f8c9bc5028640bc5712201c4a\n",
    "print(f'Voxel Dimensions: {shape_data[\"points\"].shape}')  # expected output: (3, 1024)\n",
    "print(f'Label: {shape_data[\"label\"]} | {ShapeNetPoints.classes[shape_data[\"label\"]]} | {ShapeNetPoints.class_name_mapping[ShapeNetPoints.classes[shape_data[\"label\"]]]}')  # expected output: 10, 04379243\n",
    "\n",
    "visualize_pointcloud(shape_data[\"points\"].T, point_size=0.025, flip_axes=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Defining the model\n",
    "\n",
    "The model architecture of PointNet was discussed in the lecture and is visualized below:\n",
    "<img src=\"exercise_2/images/pointnet.png\" alt=\"pointnet_architecture\" style=\"width: 800px;\"/>\n",
    "\n",
    "Some hints for the actual implementation:\n",
    "1. We use conv1d layers with kernel size 1 for all \"shared\" mlps to expand the feature channel dimension, e.g. when the input is of shape batch_size x 3 x 1024 and we apply conv1d(in_features=3, out_features=64), then we get to shape batch_size x 64 x 1024\n",
    "2. The mlps in the classification network after the max pooling operation are implemented using Linear layers\n",
    "3. The numbers in parenthesis after mlp() in the visualization above descibe the number of layers with their out_features dimension. Note though that the first mlp from nx3 to nx64 is expressed as two layers in the original tensorflow code but can be implemented as a single conv1d layer going from 3 to 64 features in the pytorch version.\n",
    "4. We define all layers up to and including the max pooling operation as the `PointNetEncoder`. The architecture of the model head depends on the task we are trying to solve: Either classification (`PointNetClassification`, used in this part of the exercise) or segmentation (`PointNetSegmentation`, used in 2.5).\n",
    "5. ReLU and Batch Norms are applied after each layer, except after the last classification layer. In the last layer before the max operation, we only apply Batch Norm but no ReLU.\n",
    "6. Dropout is applied for classification only after the second Linear layer, before the Batch Norm.\n",
    "7. The TNets are basically small PointNets.\n",
    "\n",
    "Implement the missing parts of the PointNet architecture in `TNet`, `PointNetEncoder`, and `PointNetClassification`, as indicated by the TODOs. All of them are located in `exercise_2/models/pointnet.py`. Use the following code cell to sanity check your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   | Name                                      | Type                   | Params \n",
      "---------------------------------------------------------------------------------------\n",
      "0  | encoder                                   | PointNetEncoder        | 2803529\n",
      "1  | encoder.mlp1                              | Conv1d                 | 256    \n",
      "2  | encoder.batch_norm1                       | BatchNorm1d            | 128    \n",
      "3  | encoder.mlp2                              | Conv1d                 | 8320   \n",
      "4  | encoder.batch_norm2                       | BatchNorm1d            | 256    \n",
      "5  | encoder.mlp3                              | Conv1d                 | 132096 \n",
      "6  | encoder.batch_norm3                       | BatchNorm1d            | 2048   \n",
      "7  | encoder.relu                              | ReLU                   | 0      \n",
      "8  | encoder.input_transform_net               | TNet                   | 803081 \n",
      "9  | encoder.input_transform_net.relu          | ReLU                   | 0      \n",
      "10 | encoder.input_transform_net.conv1         | Conv1d                 | 256    \n",
      "11 | encoder.input_transform_net.batch_norm1   | BatchNorm1d            | 128    \n",
      "12 | encoder.input_transform_net.conv2         | Conv1d                 | 8320   \n",
      "13 | encoder.input_transform_net.batch_norm2   | BatchNorm1d            | 256    \n",
      "14 | encoder.input_transform_net.conv3         | Conv1d                 | 132096 \n",
      "15 | encoder.input_transform_net.batch_norm3   | BatchNorm1d            | 2048   \n",
      "16 | encoder.input_transform_net.ffn1          | Linear                 | 524800 \n",
      "17 | encoder.input_transform_net.batch_norm4   | BatchNorm1d            | 1024   \n",
      "18 | encoder.input_transform_net.ffn2          | Linear                 | 131328 \n",
      "19 | encoder.input_transform_net.batch_norm5   | BatchNorm1d            | 512    \n",
      "20 | encoder.input_transform_net.ffn3          | Linear                 | 2313   \n",
      "21 | encoder.feature_transform_net             | TNet                   | 1857344\n",
      "22 | encoder.feature_transform_net.relu        | ReLU                   | 0      \n",
      "23 | encoder.feature_transform_net.conv1       | Conv1d                 | 4160   \n",
      "24 | encoder.feature_transform_net.batch_norm1 | BatchNorm1d            | 128    \n",
      "25 | encoder.feature_transform_net.conv2       | Conv1d                 | 8320   \n",
      "26 | encoder.feature_transform_net.batch_norm2 | BatchNorm1d            | 256    \n",
      "27 | encoder.feature_transform_net.conv3       | Conv1d                 | 132096 \n",
      "28 | encoder.feature_transform_net.batch_norm3 | BatchNorm1d            | 2048   \n",
      "29 | encoder.feature_transform_net.ffn1        | Linear                 | 524800 \n",
      "30 | encoder.feature_transform_net.batch_norm4 | BatchNorm1d            | 1024   \n",
      "31 | encoder.feature_transform_net.ffn2        | Linear                 | 131328 \n",
      "32 | encoder.feature_transform_net.batch_norm5 | BatchNorm1d            | 512    \n",
      "33 | encoder.feature_transform_net.ffn3        | Linear                 | 1052672\n",
      "34 | ffn1                                      | Linear                 | 524800 \n",
      "35 | batch_norm1                               | BatchNorm1d            | 1024   \n",
      "36 | ffn2                                      | Linear                 | 131328 \n",
      "37 | dropout                                   | Dropout                | 0      \n",
      "38 | batch_norm2                               | BatchNorm1d            | 512    \n",
      "39 | ffn3                                      | Linear                 | 3341   \n",
      "40 | relu                                      | ReLU                   | 0      \n",
      "41 | TOTAL                                     | PointNetClassification | 3464534\n",
      "Output tensor shape:  torch.Size([8, 13])\n",
      "Number of traininable params: 3.46M\n"
     ]
    }
   ],
   "source": [
    "from exercise_2.model.pointnet import PointNetClassification\n",
    "from exercise_2.util.model import summarize_model\n",
    "\n",
    "pointnet = PointNetClassification(13)\n",
    "print(summarize_model(pointnet))  # Expected: Rows 0-40 and TOTAL = 3464534\n",
    "\n",
    "input_tensor = torch.randn(8, 3, 1024)\n",
    "predictions = pointnet(input_tensor)\n",
    "\n",
    "print('Output tensor shape: ', predictions.shape)  # Expected: 8, 13\n",
    "num_trainable_params = sum(p.numel() for p in pointnet.parameters() if p.requires_grad) / 1e6\n",
    "print(f'Number of traininable params: {num_trainable_params:.2f}M')  # Expected: ~3M"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Training Script and Overfitting\n",
    "\n",
    "You can now go to the train script in `train_pointnet_classification.py` and fill in the missing pieces as in 2.3. Then, verify that your training works by overfitting to a few samples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "[049/00001] train_loss: 0.669\n",
      "[049/00001] val_loss: 0.277, val_accuracy: 90.625%\n",
      "[099/00001] train_loss: 0.148\n",
      "[099/00001] val_loss: 0.160, val_accuracy: 95.312%\n",
      "[149/00001] train_loss: 0.062\n",
      "[149/00001] val_loss: 0.141, val_accuracy: 95.312%\n",
      "[199/00001] train_loss: 0.040\n",
      "[199/00001] val_loss: 0.002, val_accuracy: 100.000%\n",
      "[249/00001] train_loss: 0.038\n",
      "[249/00001] val_loss: 0.002, val_accuracy: 100.000%\n",
      "[299/00001] train_loss: 0.034\n",
      "[299/00001] val_loss: 0.021, val_accuracy: 98.438%\n"
     ]
    }
   ],
   "source": [
    "from exercise_2.training import train_pointnet_classification\n",
    "config = {\n",
    "    'experiment_name': '2_4_pointnet_classification_overfitting',\n",
    "    'device': 'cuda:0',                   # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': True,                   # True since we're doing overfitting\n",
    "    'batch_size': 32,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate': 0.001,\n",
    "    'max_epochs': 300,\n",
    "    'print_every_n': 100,\n",
    "    'validate_every_n': 100,\n",
    "}\n",
    "\n",
    "train_pointnet_classification.main(config)  # should be able to get ~0 loss, 100% accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Training over the entire training set\n",
    "\n",
    "Once your overfitting completes successfully, you can move on to training on the entire dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "[000/00099] train_loss: 1.122\n",
      "[000/00199] train_loss: 0.682\n",
      "[000/00249] val_loss: 0.676, val_accuracy: 78.456%\n",
      "[000/00299] train_loss: 0.607\n",
      "[000/00399] train_loss: 0.500\n",
      "[000/00499] train_loss: 0.428\n",
      "[000/00499] val_loss: 0.389, val_accuracy: 87.910%\n",
      "[000/00599] train_loss: 0.420\n",
      "[001/00020] train_loss: 0.429\n",
      "[001/00070] val_loss: 0.360, val_accuracy: 88.555%\n",
      "[001/00120] train_loss: 0.431\n",
      "[001/00220] train_loss: 0.358\n",
      "[001/00320] train_loss: 0.343\n",
      "[001/00320] val_loss: 0.321, val_accuracy: 90.232%\n",
      "[001/00420] train_loss: 0.336\n",
      "[001/00520] train_loss: 0.336\n",
      "[001/00570] val_loss: 0.349, val_accuracy: 89.071%\n",
      "[001/00620] train_loss: 0.321\n",
      "[002/00041] train_loss: 0.333\n",
      "[002/00141] train_loss: 0.330\n",
      "[002/00141] val_loss: 0.450, val_accuracy: 83.081%\n",
      "[002/00241] train_loss: 0.328\n",
      "[002/00341] train_loss: 0.324\n",
      "[002/00391] val_loss: 0.272, val_accuracy: 91.449%\n",
      "[002/00441] train_loss: 0.298\n",
      "[002/00541] train_loss: 0.269\n",
      "[002/00641] train_loss: 0.331\n",
      "[002/00641] val_loss: 0.262, val_accuracy: 91.614%\n",
      "[003/00062] train_loss: 0.279\n",
      "[003/00162] train_loss: 0.293\n",
      "[003/00212] val_loss: 0.231, val_accuracy: 93.273%\n",
      "[003/00262] train_loss: 0.278\n",
      "[003/00362] train_loss: 0.271\n",
      "[003/00462] train_loss: 0.257\n",
      "[003/00462] val_loss: 0.268, val_accuracy: 91.596%\n",
      "[003/00562] train_loss: 0.279\n",
      "[003/00662] train_loss: 0.257\n",
      "[004/00033] val_loss: 0.293, val_accuracy: 90.619%\n",
      "[004/00083] train_loss: 0.252\n",
      "[004/00183] train_loss: 0.228\n",
      "[004/00283] train_loss: 0.241\n",
      "[004/00283] val_loss: 0.259, val_accuracy: 91.799%\n",
      "[004/00383] train_loss: 0.283\n",
      "[004/00483] train_loss: 0.262\n",
      "[004/00533] val_loss: 0.248, val_accuracy: 92.296%\n",
      "[004/00583] train_loss: 0.281\n",
      "[005/00004] train_loss: 0.242\n",
      "[005/00104] train_loss: 0.249\n",
      "[005/00104] val_loss: 0.277, val_accuracy: 91.154%\n",
      "[005/00204] train_loss: 0.256\n",
      "[005/00304] train_loss: 0.266\n",
      "[005/00354] val_loss: 0.224, val_accuracy: 93.513%\n",
      "[005/00404] train_loss: 0.220\n",
      "[005/00504] train_loss: 0.230\n",
      "[005/00604] train_loss: 0.235\n",
      "[005/00604] val_loss: 0.231, val_accuracy: 92.647%\n",
      "[006/00025] train_loss: 0.284\n",
      "[006/00125] train_loss: 0.242\n",
      "[006/00175] val_loss: 0.224, val_accuracy: 92.905%\n",
      "[006/00225] train_loss: 0.243\n",
      "[006/00325] train_loss: 0.221\n",
      "[006/00425] train_loss: 0.228\n",
      "[006/00425] val_loss: 0.221, val_accuracy: 92.923%\n",
      "[006/00525] train_loss: 0.237\n",
      "[006/00625] train_loss: 0.210\n",
      "[006/00675] val_loss: 0.225, val_accuracy: 93.255%\n",
      "[007/00046] train_loss: 0.217\n",
      "[007/00146] train_loss: 0.215\n",
      "[007/00246] train_loss: 0.228\n",
      "[007/00246] val_loss: 0.227, val_accuracy: 93.163%\n",
      "[007/00346] train_loss: 0.195\n",
      "[007/00446] train_loss: 0.218\n",
      "[007/00496] val_loss: 0.245, val_accuracy: 92.518%\n",
      "[007/00546] train_loss: 0.252\n",
      "[007/00646] train_loss: 0.224\n",
      "[008/00067] train_loss: 0.200\n",
      "[008/00067] val_loss: 0.208, val_accuracy: 93.550%\n",
      "[008/00167] train_loss: 0.197\n",
      "[008/00267] train_loss: 0.227\n",
      "[008/00317] val_loss: 0.237, val_accuracy: 92.278%\n",
      "[008/00367] train_loss: 0.225\n",
      "[008/00467] train_loss: 0.227\n",
      "[008/00567] train_loss: 0.223\n",
      "[008/00567] val_loss: 0.230, val_accuracy: 92.628%\n",
      "[008/00667] train_loss: 0.197\n",
      "[009/00088] train_loss: 0.222\n",
      "[009/00138] val_loss: 0.233, val_accuracy: 92.518%\n",
      "[009/00188] train_loss: 0.188\n",
      "[009/00288] train_loss: 0.207\n",
      "[009/00388] train_loss: 0.198\n",
      "[009/00388] val_loss: 0.236, val_accuracy: 92.223%\n",
      "[009/00488] train_loss: 0.198\n",
      "[009/00588] train_loss: 0.224\n",
      "[009/00638] val_loss: 0.215, val_accuracy: 93.328%\n"
     ]
    }
   ],
   "source": [
    "from exercise_2.training import train_pointnet_classification\n",
    "config = {\n",
    "    'experiment_name': '2_4_pointnet_classification_generalization',\n",
    "    'device': 'cuda:0',                    # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 32,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate': 0.001,\n",
    "    'max_epochs': 10,\n",
    "    'print_every_n': 100,\n",
    "    'validate_every_n': 250,\n",
    "}\n",
    "\n",
    "train_pointnet_classification.main(config)  # Should be able to get > 92% accuracy on the val set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f) Inference using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_2.inference.infer_pointnet_classification import InferenceHandlerPointNetClassification\n",
    "\n",
    "# create a handler for inference using a trained checkpoint\n",
    "inferer = InferenceHandlerPointNetClassification('exercise_2/runs/2_4_pointnet_classification_generalization/model_best.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category: chair\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39621d87e28480e9edef13dfa1ea2bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get shape point cloud and visualize\n",
    "shape_points = ShapeNetPoints.get_point_cloud('03001627/f913501826c588e89753496ba23f2183')\n",
    "print('Predicted category:', inferer.infer_single(shape_points))  # expected output: chair\n",
    "visualize_pointcloud(shape_points.T, point_size=0.025, flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category: airplane\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc8e602b4e1419a99531059ce8f9dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get shape point cloud and visualize\n",
    "shape_points = ShapeNetPoints.get_point_cloud('02691156/6af4383123972f2262b600da24e0965')\n",
    "print('Predicted category:', inferer.infer_single(shape_points))\n",
    "visualize_pointcloud(shape_points.T, point_size=0.025, flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category: rifle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d27800eb80b4e3992678fee9054e2c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get shape point cloud and visualize\n",
    "shape_points = ShapeNetPoints.get_point_cloud('04090263/eae96ddf483e896c805d3d8e378d155e')\n",
    "print('Predicted category:', inferer.infer_single(shape_points))\n",
    "visualize_pointcloud(shape_points.T, point_size=0.025, flip_axes=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you submit the trained model `exercise_2/runs/2_4_pointnet_classification_generalization/model_best.ckpt` in your zip\n",
    "so that we can evaluate it on the test set at our end."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Shape Parts Segmentation using PointNet\n",
    "\n",
    "We now go one step further: We do not just want to learn the overall class label for a given shape but instead for each point in a shape the part it belongs to. We call this Part Segmentation. The good thing is that we can actually re-use most of the PointNet architecture from 2.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Download the ShapeNetPart dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotating data for segmentation is a lot of effort since labelling has to be performed within the shape for each part instead of globally for the entire shape.\n",
    "\n",
    "Luckily, there are existing datasets we can use for this. In our case, this is the ShapeNet Part Segmenation dataset that you can download in the cell below.\n",
    "\n",
    "In terms of data layout, the general idea of shape class identifiers and shape IDs is the same; we just have slightly different shape categories now. Also, each point cloud now has a correponding file specifying the part class for every point.\n",
    "\n",
    "We put the shape class labels for this dataset in `exercise_2/data/shape_parts_info.json`, analogous to `shape_info.json` from exercise parts 2.3 and 2.4.\n",
    "\n",
    "The point cloud data is stored as pts files which is basically an even simpler version of obj. It omits the v in front of each line that represents a point and does not support faces. Each line therefore represents one point with its xyz coordinates, separated by a space.\n",
    "\n",
    "```\n",
    "# contents of exercise_2/data/shapenetcore_partanno_segmentation_benchmark_v0\n",
    "\n",
    "02691156/                                         # Shape category folder with all its shapes\n",
    "    ├── points                                    # All point clouds go here\n",
    "        ├── 1a04e3eab45ca15dd86060f189eb133.pts   # Point cloud data\n",
    "        ├── 1a32f10b20170883663e90eaf6b4ca52.pts  # Another point cloud\n",
    "        :\n",
    "        :\n",
    "    ├── points_label                              # Part labels for each point in the corresponding pts file\n",
    "        ├── 1a04e3eab45ca15dd86060f189eb133.seg   # Each line represents the local part class of a point\n",
    "        ├── 1a32f10b20170883663e90eaf6b4ca52.seg  # Another segmentation file\n",
    "        :\n",
    "        :\n",
    "    ├── seg_img                                   # Visualizations of the original mesh part segmentation\n",
    "02773838/                                         # Another shape category folder\n",
    "02954340/                                         # In total you should have 16 shape category folders\n",
    ":\n",
    ":\n",
    "train_test_split/                                 # Official split IDs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ...\n",
      "--2025-11-20 21:45:54--  https://kaldir.vc.in.tum.de/cdiller/shapenetcore_partanno_segmentation_benchmark_v0.zip\n",
      "Resolving kaldir.vc.in.tum.de (kaldir.vc.in.tum.de)... 131.159.98.128\n",
      "Connecting to kaldir.vc.in.tum.de (kaldir.vc.in.tum.de)|131.159.98.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 666265910 (635M) [application/zip]\n",
      "Saving to: ‘exercise_2/data/shapenetcore_partanno_segmentation_benchmark_v0.zip’\n",
      "\n",
      "shapenetcore_partan 100%[===================>] 635.40M  2.29MB/s    in 4m 8s   \n",
      "\n",
      "2025-11-20 21:50:02 (2.57 MB/s) - ‘exercise_2/data/shapenetcore_partanno_segmentation_benchmark_v0.zip’ saved [666265910/666265910]\n",
      "\n",
      "Extracting ...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# https://kaldir.vc.in.tum.de/cdiller/shapenetcore_partanno_segmentation_benchmark_v0.zip\n",
    "print('Downloading ...')\n",
    "!wget  https://kaldir.vc.in.tum.de/cdiller/shapenetcore_partanno_segmentation_benchmark_v0.zip --no-check-certificate -P exercise_2/data\n",
    "print('Extracting ...')\n",
    "!unzip -q exercise_2/data/shapenetcore_partanno_segmentation_benchmark_v0.zip -d exercise_2/data\n",
    "!rm exercise_2/data/shapenetcore_partanno_segmentation_benchmark_v0.zip\n",
    "print('Done.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Dataset implementation\n",
    "\n",
    "You can use the same split setup as in 2.3 and 2.4: `overfit.txt` for overfitting, `train.txt` for the train samples, and `val.txt` for the val samples; This time, use the files in folder `exercise_2/data/splits/shapenet_parts`.\n",
    "\n",
    "The dataset implementation will be similar to 2.3 and 2.4: Fill out the missing implementations of functions `__getitem__` and `__len__` in class `ShapeNetPoints` in `exercise_2/data/shapenet_parts.py`. Note that you now need to load not only the point cloud but also the per-point segmentation labels in function `get_point_cloud_with_labels`. Since each point cloud in this dataset contains more than 1024 points, we also need to sub-sample the raw points list. Use `np.random.choice` for this: Randomizing the sampling will work as augmentation which in turn helps prevent overfitting. Make sure to sample the corresponding points and labels when doing so.\n",
    "\n",
    "Once done, test your implementation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train set: 12137\n",
      "Length of val set: 1870\n",
      "Length of overfit set: 64\n"
     ]
    }
   ],
   "source": [
    "from exercise_2.data.shapenet_parts import ShapeNetParts\n",
    "\n",
    "# Create a dataset with train split\n",
    "train_dataset = ShapeNetParts('train')\n",
    "val_dataset = ShapeNetParts('val')\n",
    "overfit_dataset = ShapeNetParts('overfit')\n",
    "\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of train set: {len(train_dataset)}')  # expected output: 12137\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of val set: {len(val_dataset)}')  # expected output: 1870\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of overfit set: {len(overfit_dataset)}')  # expected output: 64"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Modifying the PointNet Model\n",
    "\n",
    "Take a look at the PointNet architecture again:\n",
    "<img src=\"exercise_2/images/pointnet.png\" alt=\"pointnet_architecture\" style=\"width: 800px;\"/>\n",
    "\n",
    "We only cared about the blue classification part in 2.4. Now, we also want to implement the yellow part. You can re-use your encoder from 2.4. \n",
    "\n",
    "The idea is simple: Take the n points with 64-dimensional point features from the correct layer of the encoder and concatenate the global shape descriptor you get after applying the max function to it. Then, implement the remaining layers as conv1ds with batchnorm and relu after all but the last layer. The final layer reduces the dimensionality per point to m which is 50 in our case since we have 50 overall parts.\n",
    "\n",
    "Add the missing layers to `PointNetSegmentation` in `exercise_2/models/pointnet.py` and finish the implementation of the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   | Name                                      | Type                 | Params \n",
      "-------------------------------------------------------------------------------------\n",
      "0  | encoder                                   | PointNetEncoder      | 2803529\n",
      "1  | encoder.mlp1                              | Conv1d               | 256    \n",
      "2  | encoder.batch_norm1                       | BatchNorm1d          | 128    \n",
      "3  | encoder.mlp2                              | Conv1d               | 8320   \n",
      "4  | encoder.batch_norm2                       | BatchNorm1d          | 256    \n",
      "5  | encoder.mlp3                              | Conv1d               | 132096 \n",
      "6  | encoder.batch_norm3                       | BatchNorm1d          | 2048   \n",
      "7  | encoder.relu                              | ReLU                 | 0      \n",
      "8  | encoder.input_transform_net               | TNet                 | 803081 \n",
      "9  | encoder.input_transform_net.relu          | ReLU                 | 0      \n",
      "10 | encoder.input_transform_net.conv1         | Conv1d               | 256    \n",
      "11 | encoder.input_transform_net.batch_norm1   | BatchNorm1d          | 128    \n",
      "12 | encoder.input_transform_net.conv2         | Conv1d               | 8320   \n",
      "13 | encoder.input_transform_net.batch_norm2   | BatchNorm1d          | 256    \n",
      "14 | encoder.input_transform_net.conv3         | Conv1d               | 132096 \n",
      "15 | encoder.input_transform_net.batch_norm3   | BatchNorm1d          | 2048   \n",
      "16 | encoder.input_transform_net.ffn1          | Linear               | 524800 \n",
      "17 | encoder.input_transform_net.batch_norm4   | BatchNorm1d          | 1024   \n",
      "18 | encoder.input_transform_net.ffn2          | Linear               | 131328 \n",
      "19 | encoder.input_transform_net.batch_norm5   | BatchNorm1d          | 512    \n",
      "20 | encoder.input_transform_net.ffn3          | Linear               | 2313   \n",
      "21 | encoder.feature_transform_net             | TNet                 | 1857344\n",
      "22 | encoder.feature_transform_net.relu        | ReLU                 | 0      \n",
      "23 | encoder.feature_transform_net.conv1       | Conv1d               | 4160   \n",
      "24 | encoder.feature_transform_net.batch_norm1 | BatchNorm1d          | 128    \n",
      "25 | encoder.feature_transform_net.conv2       | Conv1d               | 8320   \n",
      "26 | encoder.feature_transform_net.batch_norm2 | BatchNorm1d          | 256    \n",
      "27 | encoder.feature_transform_net.conv3       | Conv1d               | 132096 \n",
      "28 | encoder.feature_transform_net.batch_norm3 | BatchNorm1d          | 2048   \n",
      "29 | encoder.feature_transform_net.ffn1        | Linear               | 524800 \n",
      "30 | encoder.feature_transform_net.batch_norm4 | BatchNorm1d          | 1024   \n",
      "31 | encoder.feature_transform_net.ffn2        | Linear               | 131328 \n",
      "32 | encoder.feature_transform_net.batch_norm5 | BatchNorm1d          | 512    \n",
      "33 | encoder.feature_transform_net.ffn3        | Linear               | 1052672\n",
      "34 | conv1                                     | Conv1d               | 557568 \n",
      "35 | batch_norm1                               | BatchNorm1d          | 1024   \n",
      "36 | conv2                                     | Conv1d               | 131328 \n",
      "37 | batch_norm2                               | BatchNorm1d          | 512    \n",
      "38 | conv3                                     | Conv1d               | 32896  \n",
      "39 | batch_norm3                               | BatchNorm1d          | 256    \n",
      "40 | conv4                                     | Conv1d               | 6450   \n",
      "41 | relu                                      | ReLU                 | 0      \n",
      "42 | TOTAL                                     | PointNetSegmentation | 3533563\n",
      "Output tensor shape:  torch.Size([8, 1024, 50])\n",
      "Number of traininable params: 3.53M\n"
     ]
    }
   ],
   "source": [
    "from exercise_2.model.pointnet import PointNetSegmentation\n",
    "from exercise_2.util.model import summarize_model\n",
    "\n",
    "pointnet = PointNetSegmentation(50)\n",
    "print(summarize_model(pointnet))  # Expected: Rows 0-40 and TOTAL = 3533563\n",
    "\n",
    "input_tensor = torch.randn(8, 3, 1024)\n",
    "predictions = pointnet(input_tensor)\n",
    "\n",
    "print('Output tensor shape: ', predictions.shape)  # Expected: 8, 1024, 50\n",
    "num_trainable_params = sum(p.numel() for p in pointnet.parameters() if p.requires_grad) / 1e6\n",
    "print(f'Number of traininable params: {num_trainable_params:.2f}M')  # Expected: ~3M"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Training Script and Overfitting\n",
    "\n",
    "You can now go to the train script in `train_pointnet_segmentation.py` and fill in the missing pieces as in 2.3 and 2.4. Then, verify that your training work by overfitting to a few samples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "[006/00003] train_loss: 2.623\n",
      "[006/00003] val_loss: 1.745, val_accuracy: 54.431%, val_iou: 0.822\n",
      "[012/00007] train_loss: 1.814\n",
      "[012/00007] val_loss: 1.328, val_accuracy: 54.022%, val_iou: 0.821\n",
      "[018/00011] train_loss: 1.246\n",
      "[018/00011] val_loss: 0.998, val_accuracy: 61.536%, val_iou: 0.822\n",
      "[024/00015] train_loss: 1.071\n",
      "[024/00015] val_loss: 0.729, val_accuracy: 72.751%, val_iou: 0.865\n",
      "[031/00003] train_loss: 0.881\n",
      "[031/00003] val_loss: 0.817, val_accuracy: 73.087%, val_iou: 0.854\n",
      "[037/00007] train_loss: 0.842\n",
      "[037/00007] val_loss: 0.518, val_accuracy: 81.558%, val_iou: 0.907\n",
      "[043/00011] train_loss: 0.583\n",
      "[043/00011] val_loss: 0.402, val_accuracy: 84.662%, val_iou: 0.917\n",
      "[049/00015] train_loss: 0.416\n",
      "[049/00015] val_loss: 0.470, val_accuracy: 83.636%, val_iou: 0.913\n",
      "[056/00003] train_loss: 0.701\n",
      "[056/00003] val_loss: 0.799, val_accuracy: 75.021%, val_iou: 0.879\n",
      "[062/00007] train_loss: 0.592\n",
      "[062/00007] val_loss: 0.426, val_accuracy: 83.521%, val_iou: 0.916\n",
      "[068/00011] train_loss: 0.539\n",
      "[068/00011] val_loss: 0.755, val_accuracy: 73.770%, val_iou: 0.871\n",
      "[074/00015] train_loss: 0.912\n",
      "[074/00015] val_loss: 0.486, val_accuracy: 82.108%, val_iou: 0.911\n",
      "[081/00003] train_loss: 0.380\n",
      "[081/00003] val_loss: 0.356, val_accuracy: 85.922%, val_iou: 0.932\n",
      "[087/00007] train_loss: 0.573\n",
      "[087/00007] val_loss: 0.449, val_accuracy: 82.526%, val_iou: 0.905\n",
      "[093/00011] train_loss: 0.452\n",
      "[093/00011] val_loss: 0.322, val_accuracy: 87.137%, val_iou: 0.930\n",
      "[099/00015] train_loss: 0.301\n",
      "[099/00015] val_loss: 0.271, val_accuracy: 88.895%, val_iou: 0.944\n",
      "[106/00003] train_loss: 0.274\n",
      "[106/00003] val_loss: 0.264, val_accuracy: 89.313%, val_iou: 0.948\n",
      "[112/00007] train_loss: 0.265\n",
      "[112/00007] val_loss: 0.255, val_accuracy: 90.121%, val_iou: 0.952\n",
      "[118/00011] train_loss: 0.262\n",
      "[118/00011] val_loss: 0.262, val_accuracy: 88.800%, val_iou: 0.946\n",
      "[124/00015] train_loss: 0.256\n",
      "[124/00015] val_loss: 0.252, val_accuracy: 89.838%, val_iou: 0.954\n",
      "[131/00003] train_loss: 0.245\n",
      "[131/00003] val_loss: 0.221, val_accuracy: 90.665%, val_iou: 0.956\n",
      "[137/00007] train_loss: 0.228\n",
      "[137/00007] val_loss: 0.211, val_accuracy: 91.602%, val_iou: 0.959\n",
      "[143/00011] train_loss: 0.224\n",
      "[143/00011] val_loss: 0.207, val_accuracy: 91.748%, val_iou: 0.961\n",
      "[149/00015] train_loss: 0.226\n",
      "[149/00015] val_loss: 0.235, val_accuracy: 89.529%, val_iou: 0.952\n",
      "[156/00003] train_loss: 0.223\n",
      "[156/00003] val_loss: 0.210, val_accuracy: 91.565%, val_iou: 0.957\n",
      "[162/00007] train_loss: 1.226\n",
      "[162/00007] val_loss: 0.665, val_accuracy: 72.760%, val_iou: 0.869\n",
      "[168/00011] train_loss: 0.464\n",
      "[168/00011] val_loss: 0.331, val_accuracy: 86.856%, val_iou: 0.931\n",
      "[174/00015] train_loss: 0.307\n",
      "[174/00015] val_loss: 0.256, val_accuracy: 89.957%, val_iou: 0.945\n",
      "[181/00003] train_loss: 0.240\n",
      "[181/00003] val_loss: 0.223, val_accuracy: 90.726%, val_iou: 0.956\n",
      "[187/00007] train_loss: 0.235\n",
      "[187/00007] val_loss: 0.223, val_accuracy: 91.339%, val_iou: 0.958\n",
      "[193/00011] train_loss: 0.372\n",
      "[193/00011] val_loss: 1.286, val_accuracy: 71.838%, val_iou: 0.860\n",
      "[199/00015] train_loss: 1.065\n",
      "[199/00015] val_loss: 0.791, val_accuracy: 71.942%, val_iou: 0.841\n",
      "[206/00003] train_loss: 0.676\n",
      "[206/00003] val_loss: 0.423, val_accuracy: 86.063%, val_iou: 0.915\n",
      "[212/00007] train_loss: 0.505\n",
      "[212/00007] val_loss: 0.416, val_accuracy: 84.271%, val_iou: 0.913\n",
      "[218/00011] train_loss: 0.296\n",
      "[218/00011] val_loss: 0.222, val_accuracy: 91.299%, val_iou: 0.951\n",
      "[224/00015] train_loss: 0.251\n",
      "[224/00015] val_loss: 0.238, val_accuracy: 89.792%, val_iou: 0.943\n",
      "[231/00003] train_loss: 0.221\n",
      "[231/00003] val_loss: 0.195, val_accuracy: 91.983%, val_iou: 0.964\n",
      "[237/00007] train_loss: 0.198\n",
      "[237/00007] val_loss: 0.195, val_accuracy: 92.218%, val_iou: 0.964\n",
      "[243/00011] train_loss: 0.190\n",
      "[243/00011] val_loss: 0.183, val_accuracy: 93.216%, val_iou: 0.966\n",
      "[249/00015] train_loss: 0.209\n",
      "[249/00015] val_loss: 0.225, val_accuracy: 91.599%, val_iou: 0.955\n",
      "[256/00003] train_loss: 0.398\n",
      "[256/00003] val_loss: 0.343, val_accuracy: 88.156%, val_iou: 0.925\n",
      "[262/00007] train_loss: 0.283\n",
      "[262/00007] val_loss: 0.210, val_accuracy: 91.547%, val_iou: 0.955\n",
      "[268/00011] train_loss: 0.200\n",
      "[268/00011] val_loss: 0.218, val_accuracy: 91.425%, val_iou: 0.959\n",
      "[274/00015] train_loss: 0.196\n",
      "[274/00015] val_loss: 0.173, val_accuracy: 93.320%, val_iou: 0.967\n",
      "[281/00003] train_loss: 0.169\n",
      "[281/00003] val_loss: 0.161, val_accuracy: 93.182%, val_iou: 0.965\n",
      "[287/00007] train_loss: 0.164\n",
      "[287/00007] val_loss: 0.174, val_accuracy: 93.204%, val_iou: 0.963\n",
      "[293/00011] train_loss: 0.159\n",
      "[293/00011] val_loss: 0.154, val_accuracy: 93.594%, val_iou: 0.966\n",
      "[299/00015] train_loss: 0.199\n",
      "[299/00015] val_loss: 1.240, val_accuracy: 81.396%, val_iou: 0.920\n",
      "[306/00003] train_loss: 0.586\n",
      "[306/00003] val_loss: 0.277, val_accuracy: 88.660%, val_iou: 0.944\n",
      "[312/00007] train_loss: 0.384\n",
      "[312/00007] val_loss: 0.607, val_accuracy: 82.153%, val_iou: 0.885\n",
      "[318/00011] train_loss: 0.356\n",
      "[318/00011] val_loss: 0.178, val_accuracy: 92.606%, val_iou: 0.959\n",
      "[324/00015] train_loss: 0.161\n",
      "[324/00015] val_loss: 0.147, val_accuracy: 94.067%, val_iou: 0.965\n",
      "[331/00003] train_loss: 0.153\n",
      "[331/00003] val_loss: 0.162, val_accuracy: 93.243%, val_iou: 0.965\n",
      "[337/00007] train_loss: 0.146\n",
      "[337/00007] val_loss: 0.122, val_accuracy: 95.166%, val_iou: 0.974\n",
      "[343/00011] train_loss: 0.132\n",
      "[343/00011] val_loss: 0.125, val_accuracy: 94.852%, val_iou: 0.970\n",
      "[349/00015] train_loss: 0.134\n",
      "[349/00015] val_loss: 0.119, val_accuracy: 95.227%, val_iou: 0.974\n",
      "[356/00003] train_loss: 0.125\n",
      "[356/00003] val_loss: 0.117, val_accuracy: 95.233%, val_iou: 0.973\n",
      "[362/00007] train_loss: 0.123\n",
      "[362/00007] val_loss: 0.125, val_accuracy: 95.056%, val_iou: 0.973\n",
      "[368/00011] train_loss: 0.120\n",
      "[368/00011] val_loss: 0.103, val_accuracy: 95.923%, val_iou: 0.977\n",
      "[374/00015] train_loss: 0.136\n",
      "[374/00015] val_loss: 0.153, val_accuracy: 93.988%, val_iou: 0.970\n",
      "[381/00003] train_loss: 0.439\n",
      "[381/00003] val_loss: 0.587, val_accuracy: 79.919%, val_iou: 0.884\n",
      "[387/00007] train_loss: 0.440\n",
      "[387/00007] val_loss: 0.243, val_accuracy: 89.575%, val_iou: 0.952\n",
      "[393/00011] train_loss: 0.182\n",
      "[393/00011] val_loss: 0.150, val_accuracy: 93.997%, val_iou: 0.966\n",
      "[399/00015] train_loss: 0.124\n",
      "[399/00015] val_loss: 0.106, val_accuracy: 95.844%, val_iou: 0.978\n",
      "[406/00003] train_loss: 0.112\n",
      "[406/00003] val_loss: 0.099, val_accuracy: 96.069%, val_iou: 0.978\n",
      "[412/00007] train_loss: 0.146\n",
      "[412/00007] val_loss: 0.121, val_accuracy: 95.306%, val_iou: 0.972\n",
      "[418/00011] train_loss: 0.106\n",
      "[418/00011] val_loss: 0.105, val_accuracy: 95.743%, val_iou: 0.979\n",
      "[424/00015] train_loss: 0.111\n",
      "[424/00015] val_loss: 0.096, val_accuracy: 96.182%, val_iou: 0.980\n",
      "[431/00003] train_loss: 0.095\n",
      "[431/00003] val_loss: 0.092, val_accuracy: 96.384%, val_iou: 0.981\n",
      "[437/00007] train_loss: 0.098\n",
      "[437/00007] val_loss: 0.112, val_accuracy: 95.703%, val_iou: 0.977\n",
      "[443/00011] train_loss: 0.117\n",
      "[443/00011] val_loss: 0.098, val_accuracy: 96.176%, val_iou: 0.981\n",
      "[449/00015] train_loss: 0.092\n",
      "[449/00015] val_loss: 0.089, val_accuracy: 96.539%, val_iou: 0.979\n",
      "[456/00003] train_loss: 0.091\n",
      "[456/00003] val_loss: 0.089, val_accuracy: 96.735%, val_iou: 0.983\n",
      "[462/00007] train_loss: 0.093\n",
      "[462/00007] val_loss: 0.093, val_accuracy: 96.191%, val_iou: 0.981\n",
      "[468/00011] train_loss: 0.092\n",
      "[468/00011] val_loss: 0.093, val_accuracy: 96.405%, val_iou: 0.981\n",
      "[474/00015] train_loss: 0.088\n",
      "[474/00015] val_loss: 0.102, val_accuracy: 96.045%, val_iou: 0.981\n",
      "[481/00003] train_loss: 0.091\n",
      "[481/00003] val_loss: 0.075, val_accuracy: 96.967%, val_iou: 0.985\n",
      "[487/00007] train_loss: 0.092\n",
      "[487/00007] val_loss: 0.085, val_accuracy: 96.539%, val_iou: 0.982\n",
      "[493/00011] train_loss: 0.087\n",
      "[493/00011] val_loss: 0.094, val_accuracy: 96.274%, val_iou: 0.978\n",
      "[499/00015] train_loss: 0.094\n",
      "[499/00015] val_loss: 0.083, val_accuracy: 96.722%, val_iou: 0.985\n",
      "[506/00003] train_loss: 0.080\n",
      "[506/00003] val_loss: 0.071, val_accuracy: 97.034%, val_iou: 0.983\n",
      "[512/00007] train_loss: 0.087\n",
      "[512/00007] val_loss: 0.082, val_accuracy: 96.707%, val_iou: 0.981\n",
      "[518/00011] train_loss: 0.074\n",
      "[518/00011] val_loss: 0.078, val_accuracy: 96.906%, val_iou: 0.984\n",
      "[524/00015] train_loss: 0.078\n",
      "[524/00015] val_loss: 0.067, val_accuracy: 97.290%, val_iou: 0.987\n",
      "[531/00003] train_loss: 0.096\n",
      "[531/00003] val_loss: 0.131, val_accuracy: 94.647%, val_iou: 0.967\n",
      "[537/00007] train_loss: 0.566\n",
      "[537/00007] val_loss: 0.183, val_accuracy: 93.170%, val_iou: 0.953\n",
      "[543/00011] train_loss: 0.132\n",
      "[543/00011] val_loss: 0.096, val_accuracy: 96.292%, val_iou: 0.979\n",
      "[549/00015] train_loss: 0.097\n",
      "[549/00015] val_loss: 0.092, val_accuracy: 96.375%, val_iou: 0.982\n",
      "[556/00003] train_loss: 0.088\n",
      "[556/00003] val_loss: 0.083, val_accuracy: 96.686%, val_iou: 0.982\n",
      "[562/00007] train_loss: 0.077\n",
      "[562/00007] val_loss: 0.080, val_accuracy: 96.640%, val_iou: 0.981\n",
      "[568/00011] train_loss: 0.084\n",
      "[568/00011] val_loss: 0.102, val_accuracy: 95.850%, val_iou: 0.974\n",
      "[574/00015] train_loss: 0.089\n",
      "[574/00015] val_loss: 0.082, val_accuracy: 96.631%, val_iou: 0.982\n",
      "[581/00003] train_loss: 0.078\n",
      "[581/00003] val_loss: 0.072, val_accuracy: 97.070%, val_iou: 0.984\n",
      "[587/00007] train_loss: 0.074\n",
      "[587/00007] val_loss: 0.110, val_accuracy: 95.761%, val_iou: 0.978\n",
      "[593/00011] train_loss: 0.081\n",
      "[593/00011] val_loss: 0.074, val_accuracy: 97.009%, val_iou: 0.984\n",
      "[599/00015] train_loss: 0.073\n",
      "[599/00015] val_loss: 0.070, val_accuracy: 97.134%, val_iou: 0.983\n",
      "[606/00003] train_loss: 0.075\n",
      "[606/00003] val_loss: 0.076, val_accuracy: 96.872%, val_iou: 0.984\n",
      "[612/00007] train_loss: 0.185\n",
      "[612/00007] val_loss: 0.276, val_accuracy: 90.573%, val_iou: 0.944\n",
      "[618/00011] train_loss: 0.437\n",
      "[618/00011] val_loss: 0.329, val_accuracy: 88.257%, val_iou: 0.916\n",
      "[624/00015] train_loss: 0.135\n",
      "[624/00015] val_loss: 0.084, val_accuracy: 96.661%, val_iou: 0.982\n",
      "[631/00003] train_loss: 0.081\n",
      "[631/00003] val_loss: 0.089, val_accuracy: 96.448%, val_iou: 0.983\n",
      "[637/00007] train_loss: 0.085\n",
      "[637/00007] val_loss: 0.092, val_accuracy: 96.420%, val_iou: 0.978\n",
      "[643/00011] train_loss: 0.076\n",
      "[643/00011] val_loss: 0.070, val_accuracy: 97.141%, val_iou: 0.986\n",
      "[649/00015] train_loss: 0.069\n",
      "[649/00015] val_loss: 0.070, val_accuracy: 97.153%, val_iou: 0.984\n",
      "[656/00003] train_loss: 0.070\n",
      "[656/00003] val_loss: 0.066, val_accuracy: 97.287%, val_iou: 0.986\n",
      "[662/00007] train_loss: 0.070\n",
      "[662/00007] val_loss: 0.069, val_accuracy: 97.189%, val_iou: 0.985\n",
      "[668/00011] train_loss: 0.075\n",
      "[668/00011] val_loss: 0.079, val_accuracy: 96.881%, val_iou: 0.984\n",
      "[674/00015] train_loss: 0.074\n",
      "[674/00015] val_loss: 0.063, val_accuracy: 97.348%, val_iou: 0.987\n",
      "[681/00003] train_loss: 0.065\n",
      "[681/00003] val_loss: 0.067, val_accuracy: 97.162%, val_iou: 0.984\n",
      "[687/00007] train_loss: 0.071\n",
      "[687/00007] val_loss: 0.066, val_accuracy: 97.177%, val_iou: 0.986\n",
      "[693/00011] train_loss: 0.069\n",
      "[693/00011] val_loss: 0.063, val_accuracy: 97.455%, val_iou: 0.987\n",
      "[699/00015] train_loss: 0.065\n",
      "[699/00015] val_loss: 0.063, val_accuracy: 97.437%, val_iou: 0.986\n",
      "[706/00003] train_loss: 0.076\n",
      "[706/00003] val_loss: 0.067, val_accuracy: 97.141%, val_iou: 0.985\n",
      "[712/00007] train_loss: 0.075\n",
      "[712/00007] val_loss: 0.057, val_accuracy: 97.571%, val_iou: 0.988\n",
      "[718/00011] train_loss: 0.075\n",
      "[718/00011] val_loss: 0.075, val_accuracy: 97.067%, val_iou: 0.985\n",
      "[724/00015] train_loss: 0.067\n",
      "[724/00015] val_loss: 0.069, val_accuracy: 97.180%, val_iou: 0.986\n",
      "[731/00003] train_loss: 0.060\n",
      "[731/00003] val_loss: 0.059, val_accuracy: 97.543%, val_iou: 0.987\n",
      "[737/00007] train_loss: 0.068\n",
      "[737/00007] val_loss: 0.071, val_accuracy: 97.141%, val_iou: 0.986\n",
      "[743/00011] train_loss: 0.100\n",
      "[743/00011] val_loss: 0.142, val_accuracy: 94.678%, val_iou: 0.965\n",
      "[749/00015] train_loss: 0.474\n",
      "[749/00015] val_loss: 0.175, val_accuracy: 93.658%, val_iou: 0.960\n",
      "[756/00003] train_loss: 0.112\n",
      "[756/00003] val_loss: 0.075, val_accuracy: 97.043%, val_iou: 0.982\n",
      "[762/00007] train_loss: 0.074\n",
      "[762/00007] val_loss: 0.073, val_accuracy: 97.079%, val_iou: 0.986\n",
      "[768/00011] train_loss: 0.066\n",
      "[768/00011] val_loss: 0.075, val_accuracy: 96.860%, val_iou: 0.981\n",
      "[774/00015] train_loss: 0.070\n",
      "[774/00015] val_loss: 0.061, val_accuracy: 97.589%, val_iou: 0.988\n",
      "[781/00003] train_loss: 0.067\n",
      "[781/00003] val_loss: 0.067, val_accuracy: 97.110%, val_iou: 0.987\n",
      "[787/00007] train_loss: 0.065\n",
      "[787/00007] val_loss: 0.055, val_accuracy: 97.693%, val_iou: 0.989\n",
      "[793/00011] train_loss: 0.066\n",
      "[793/00011] val_loss: 0.077, val_accuracy: 96.902%, val_iou: 0.982\n",
      "[799/00015] train_loss: 0.070\n",
      "[799/00015] val_loss: 0.061, val_accuracy: 97.488%, val_iou: 0.986\n",
      "[806/00003] train_loss: 0.059\n",
      "[806/00003] val_loss: 0.060, val_accuracy: 97.455%, val_iou: 0.988\n",
      "[812/00007] train_loss: 0.068\n",
      "[812/00007] val_loss: 0.063, val_accuracy: 97.433%, val_iou: 0.984\n",
      "[818/00011] train_loss: 0.057\n",
      "[818/00011] val_loss: 0.053, val_accuracy: 97.836%, val_iou: 0.988\n",
      "[824/00015] train_loss: 0.060\n",
      "[824/00015] val_loss: 0.060, val_accuracy: 97.498%, val_iou: 0.987\n",
      "[831/00003] train_loss: 0.066\n",
      "[831/00003] val_loss: 0.056, val_accuracy: 97.650%, val_iou: 0.988\n",
      "[837/00007] train_loss: 0.059\n",
      "[837/00007] val_loss: 0.050, val_accuracy: 97.974%, val_iou: 0.990\n",
      "[843/00011] train_loss: 0.056\n",
      "[843/00011] val_loss: 0.059, val_accuracy: 97.543%, val_iou: 0.987\n",
      "[849/00015] train_loss: 0.073\n",
      "[849/00015] val_loss: 0.082, val_accuracy: 96.707%, val_iou: 0.984\n",
      "[856/00003] train_loss: 0.066\n",
      "[856/00003] val_loss: 0.063, val_accuracy: 97.412%, val_iou: 0.986\n",
      "[862/00007] train_loss: 0.059\n",
      "[862/00007] val_loss: 0.059, val_accuracy: 97.601%, val_iou: 0.986\n",
      "[868/00011] train_loss: 0.065\n",
      "[868/00011] val_loss: 0.052, val_accuracy: 97.882%, val_iou: 0.989\n",
      "[874/00015] train_loss: 0.055\n",
      "[874/00015] val_loss: 0.066, val_accuracy: 97.324%, val_iou: 0.987\n",
      "[881/00003] train_loss: 0.064\n",
      "[881/00003] val_loss: 0.054, val_accuracy: 97.794%, val_iou: 0.988\n",
      "[887/00007] train_loss: 0.063\n",
      "[887/00007] val_loss: 0.056, val_accuracy: 97.696%, val_iou: 0.989\n",
      "[893/00011] train_loss: 0.058\n",
      "[893/00011] val_loss: 0.056, val_accuracy: 97.736%, val_iou: 0.983\n",
      "[899/00015] train_loss: 0.063\n",
      "[899/00015] val_loss: 0.052, val_accuracy: 97.900%, val_iou: 0.988\n",
      "[906/00003] train_loss: 0.054\n",
      "[906/00003] val_loss: 0.067, val_accuracy: 97.430%, val_iou: 0.988\n",
      "[912/00007] train_loss: 0.064\n",
      "[912/00007] val_loss: 0.055, val_accuracy: 97.714%, val_iou: 0.989\n",
      "[918/00011] train_loss: 0.059\n",
      "[918/00011] val_loss: 0.046, val_accuracy: 98.007%, val_iou: 0.990\n",
      "[924/00015] train_loss: 0.065\n",
      "[924/00015] val_loss: 0.057, val_accuracy: 97.620%, val_iou: 0.988\n",
      "[931/00003] train_loss: 0.054\n",
      "[931/00003] val_loss: 0.047, val_accuracy: 97.977%, val_iou: 0.989\n",
      "[937/00007] train_loss: 0.063\n",
      "[937/00007] val_loss: 0.059, val_accuracy: 97.620%, val_iou: 0.987\n",
      "[943/00011] train_loss: 0.540\n",
      "[943/00011] val_loss: 0.199, val_accuracy: 93.759%, val_iou: 0.950\n",
      "[949/00015] train_loss: 0.136\n",
      "[949/00015] val_loss: 0.076, val_accuracy: 97.009%, val_iou: 0.983\n",
      "[956/00003] train_loss: 0.072\n",
      "[956/00003] val_loss: 0.068, val_accuracy: 97.220%, val_iou: 0.985\n",
      "[962/00007] train_loss: 0.060\n",
      "[962/00007] val_loss: 0.052, val_accuracy: 97.943%, val_iou: 0.989\n",
      "[968/00011] train_loss: 0.062\n",
      "[968/00011] val_loss: 0.056, val_accuracy: 97.705%, val_iou: 0.988\n",
      "[974/00015] train_loss: 0.063\n",
      "[974/00015] val_loss: 0.059, val_accuracy: 97.662%, val_iou: 0.987\n",
      "[981/00003] train_loss: 0.064\n",
      "[981/00003] val_loss: 0.053, val_accuracy: 97.852%, val_iou: 0.989\n",
      "[987/00007] train_loss: 0.056\n",
      "[987/00007] val_loss: 0.063, val_accuracy: 97.409%, val_iou: 0.986\n",
      "[993/00011] train_loss: 0.068\n",
      "[993/00011] val_loss: 0.053, val_accuracy: 97.791%, val_iou: 0.987\n",
      "[999/00015] train_loss: 0.049\n",
      "[999/00015] val_loss: 0.046, val_accuracy: 98.065%, val_iou: 0.990\n"
     ]
    }
   ],
   "source": [
    "from exercise_2.training import train_pointnet_segmentation\n",
    "config = {\n",
    "    'experiment_name': '2_5_pointnet_segmentation_overfitting',\n",
    "    'device': 'cuda:0',                   # change this to cpu if you do not have a GPU\n",
    "    # 'device': 'cpu',                   # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': True,                   # True since we're doing overfitting\n",
    "    'batch_size': 4,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate': 0.001,\n",
    "    'max_epochs': 1000,\n",
    "    'print_every_n': 100,\n",
    "    'validate_every_n': 100,\n",
    "}\n",
    "\n",
    "train_pointnet_segmentation.main(config)  # should be able to get ~0.03 loss, >97% accuracy, >0.95 iou"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Training over the entire training set\n",
    "\n",
    "Once your overfitting completes successfully, you can move on to training on the entire dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "[000/00099] train_loss: 1.487\n",
      "[000/00199] train_loss: 0.870\n",
      "[000/00249] val_loss: 0.579, val_accuracy: 81.138%, val_iou: 0.683\n",
      "[000/00299] train_loss: 0.890\n",
      "[001/00019] train_loss: 0.688\n",
      "[001/00119] train_loss: 0.551\n",
      "[001/00119] val_loss: 0.466, val_accuracy: 85.211%, val_iou: 0.722\n",
      "[001/00219] train_loss: 0.578\n",
      "[001/00319] train_loss: 0.545\n",
      "[001/00369] val_loss: 0.425, val_accuracy: 86.388%, val_iou: 0.741\n",
      "[002/00039] train_loss: 0.493\n",
      "[002/00139] train_loss: 0.468\n",
      "[002/00239] train_loss: 0.455\n",
      "[002/00239] val_loss: 0.470, val_accuracy: 83.907%, val_iou: 0.723\n",
      "[002/00339] train_loss: 0.434\n",
      "[003/00059] train_loss: 0.409\n",
      "[003/00109] val_loss: 0.367, val_accuracy: 87.552%, val_iou: 0.772\n",
      "[003/00159] train_loss: 0.408\n",
      "[003/00259] train_loss: 0.405\n",
      "[003/00359] train_loss: 0.401\n",
      "[003/00359] val_loss: 0.392, val_accuracy: 87.371%, val_iou: 0.762\n",
      "[004/00079] train_loss: 0.395\n",
      "[004/00179] train_loss: 0.394\n",
      "[004/00229] val_loss: 0.356, val_accuracy: 88.310%, val_iou: 0.780\n",
      "[004/00279] train_loss: 0.366\n",
      "[004/00379] train_loss: 0.377\n",
      "[005/00099] train_loss: 0.348\n",
      "[005/00099] val_loss: 0.323, val_accuracy: 89.033%, val_iou: 0.783\n",
      "[005/00199] train_loss: 0.361\n",
      "[005/00299] train_loss: 0.393\n",
      "[005/00349] val_loss: 0.332, val_accuracy: 88.973%, val_iou: 0.784\n",
      "[006/00019] train_loss: 0.357\n",
      "[006/00119] train_loss: 0.339\n",
      "[006/00219] train_loss: 0.357\n",
      "[006/00219] val_loss: 0.316, val_accuracy: 89.359%, val_iou: 0.797\n",
      "[006/00319] train_loss: 0.320\n",
      "[007/00039] train_loss: 0.355\n",
      "[007/00089] val_loss: 0.371, val_accuracy: 87.772%, val_iou: 0.778\n",
      "[007/00139] train_loss: 0.347\n",
      "[007/00239] train_loss: 0.347\n",
      "[007/00339] train_loss: 0.362\n",
      "[007/00339] val_loss: 0.329, val_accuracy: 89.188%, val_iou: 0.787\n",
      "[008/00059] train_loss: 0.308\n",
      "[008/00159] train_loss: 0.338\n",
      "[008/00209] val_loss: 0.313, val_accuracy: 89.366%, val_iou: 0.805\n",
      "[008/00259] train_loss: 0.305\n",
      "[008/00359] train_loss: 0.352\n",
      "[009/00079] train_loss: 0.301\n",
      "[009/00079] val_loss: 0.292, val_accuracy: 90.184%, val_iou: 0.818\n",
      "[009/00179] train_loss: 0.295\n",
      "[009/00279] train_loss: 0.307\n",
      "[009/00329] val_loss: 0.294, val_accuracy: 89.878%, val_iou: 0.807\n",
      "[009/00379] train_loss: 0.327\n",
      "[010/00099] train_loss: 0.268\n",
      "[010/00199] train_loss: 0.290\n",
      "[010/00199] val_loss: 0.300, val_accuracy: 90.124%, val_iou: 0.800\n",
      "[010/00299] train_loss: 0.299\n",
      "[011/00019] train_loss: 0.325\n",
      "[011/00069] val_loss: 0.294, val_accuracy: 90.029%, val_iou: 0.804\n",
      "[011/00119] train_loss: 0.292\n",
      "[011/00219] train_loss: 0.303\n",
      "[011/00319] train_loss: 0.270\n",
      "[011/00319] val_loss: 0.276, val_accuracy: 90.911%, val_iou: 0.825\n"
     ]
    }
   ],
   "source": [
    "from exercise_2.training import train_pointnet_segmentation\n",
    "config = {\n",
    "    'experiment_name': '2_5_pointnet_segmentation_generalization',\n",
    "    'device': 'cuda:0',                   # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 32,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate': 0.001,\n",
    "    'max_epochs': 12,\n",
    "    'print_every_n': 100,\n",
    "    'validate_every_n': 250,\n",
    "}\n",
    "\n",
    "train_pointnet_segmentation.main(config)  # Should be able to get > 90% accuracy and > 0.8 iou on the val set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f) Inference using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_2.inference.infer_pointnet_segmentation import InferenceHandlerPointNetSegmentation\n",
    "from exercise_2.util.visualization import visualize_pointcloud\n",
    "from matplotlib import cm, colors\n",
    "import numpy as np\n",
    "\n",
    "# create a handler for inference using a trained checkpoint\n",
    "inferer = InferenceHandlerPointNetSegmentation('exercise_2/runs/2_5_pointnet_segmentation_generalization/model_best.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18318/2695202539.py:5: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  point_colors = cm.get_cmap('hsv')(point_labels)[:, :3]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0038f649b1334a22a46370527a8a28af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get shape point cloud, predict labels, and visualize colored point cloud\n",
    "shape_points = ShapeNetParts.get_point_cloud_with_labels('02691156/1c4b8662938adf41da2b0f839aba40f9')[0]\n",
    "point_labels = inferer.infer_single(shape_points.T)\n",
    "point_labels = (point_labels - min(point_labels)) / (max(point_labels) - min(point_labels))\n",
    "point_colors = cm.get_cmap('hsv')(point_labels)[:, :3]\n",
    "point_colors = np.sum((point_colors * 255).astype(int) * [255*255, 255, 1], axis=1)\n",
    "visualize_pointcloud(shape_points.T, colors=point_colors, point_size=0.025, flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18318/445198810.py:5: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  point_colors = cm.get_cmap('hsv')(point_labels)[:, :3]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0013a3a2ed4880a5815902d25fe3aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get shape point cloud, predict labels, and visualize colored point cloud\n",
    "shape_points = ShapeNetParts.get_point_cloud_with_labels('03948459/e017cf5dac1e39b013d74211a209ce')[0]\n",
    "point_labels = inferer.infer_single(shape_points.T)\n",
    "point_labels = (point_labels - min(point_labels)) / (max(point_labels) - min(point_labels))\n",
    "point_colors = cm.get_cmap('hsv')(point_labels)[:, :3]\n",
    "point_colors = np.sum((point_colors * 255).astype(int) * [255*255, 255, 1], axis=1)\n",
    "visualize_pointcloud(shape_points.T, colors=point_colors, point_size=0.025, flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18318/2180876740.py:5: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  point_colors = cm.get_cmap('hsv')(point_labels)[:, :3]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b83d0c2193f4684a22bdb101a9534f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get shape point cloud, predict labels, and visualize colored point cloud\n",
    "shape_points = ShapeNetParts.get_point_cloud_with_labels('03790512/86b6dc954e1ca8e948272812609617e2')[0]\n",
    "point_labels = inferer.infer_single(shape_points.T)\n",
    "point_labels = (point_labels - min(point_labels)) / (max(point_labels) - min(point_labels))\n",
    "point_colors = cm.get_cmap('hsv')(point_labels)[:, :3]\n",
    "point_colors = np.sum((point_colors * 255).astype(int) * [255*255, 255, 1], axis=1)\n",
    "visualize_pointcloud(shape_points.T, colors=point_colors, point_size=0.025, flip_axes=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you submit the trained model `exercise_2/runs/2_5_pointnet_segmentation_generalization/model_best.ckpt` in your zip so that we can evaluate it on the test set at our end."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "This is the end of exercise 2 🙂. Please create a zip containing all files we provided, everything you modified, and all of your generated output/visualization files, including your checkpoints. Please **don't** include the downloaded data. Name it with your matriculation number(s) as described in exercise 1. Make sure this notebook can be run without problems. Then, submit via Moodle. If the zip file is too large to upload, you can split it into multiple parts.\n",
    "\n",
    "**Submission Deadline**: 02.12.2025, 23:55"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Qi, C. et al. “Volumetric and Multi-view CNNs for Object Classification on 3D Data.” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016): 5648-5656.\n",
    "\n",
    "[2] Qi, C. et al. “PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation.” 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017): 77-85."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
